{"type":"file-history-snapshot","messageId":"5313c0c0-1a04-4a66-98cb-d464b701ac2f","snapshot":{"messageId":"5313c0c0-1a04-4a66-98cb-d464b701ac2f","trackedFileBackups":{},"timestamp":"2026-02-01T03:16:05.934Z"},"isSnapshotUpdate":false}
{"type":"file-history-snapshot","messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","type":"user","message":{"role":"user","content":"Build a Prompt Engineering toolkit for PromptWizz.com\n\nPromptWizz is Matt's AI prompt optimizer tool. Build utilities:\n\n1. Prompt Analyzer (analyzer.py):\n   - Score prompts for clarity, specificity, structure\n   - Identify weaknesses (vague terms, missing context)\n   - Suggest improvements\n\n2. Prompt Templates (templates.py):\n   - Library of optimized prompt templates\n   - Categories: coding, writing, analysis, creative\n   - Variable substitution system\n\n3. A/B Tester (ab_test.py):\n   - Run same query with different prompts\n   - Compare outputs for quality\n   - Track which prompts perform better\n\n4. CLI (cli.py) tying it all together\n\nUse anthropic library for Claude API calls.\nMake it professional and well-documented.\n\nWhen done: openclaw gateway wake --text 'Done: PromptWizz toolkit' --mode now"},"uuid":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","timestamp":"2026-02-01T03:16:06.085Z","thinkingMetadata":{"maxThinkingTokens":31999},"todos":[],"permissionMode":"acceptEdits"}
{"parentUuid":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","message":{"model":"claude-opus-4-5-20251101","id":"msg_017R7UEKDjQkrsHdjSZBxV7X","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user wants me to build a Prompt Engineering toolkit for PromptWizz.com. This is a multi-file project that includes:\n\n1. Prompt Analyzer (analyzer.py) - Score prompts, identify weaknesses, suggest improvements\n2. Prompt Templates (templates.py) - Library of templates with variable substitution\n3. A/B Tester (ab_test.py) - Compare prompts and track performance\n4. CLI (cli.py) - Ties everything together\n\nI should use the anthropic library for Claude API calls. Let me plan this out and then build the files.\n\nFirst, let me check what's in the current directory to understand the project structure.","signature":"EoIGCkYICxgCKkA7LQ2j111exSsLhcQu7IpB00lSRLzCCwwkZVHqttSzhJQz13YpQ9ibhNo1iDUfB/sMdcuFh9lA+n9a0AqnrIdDEgyzxNCUnTSAmi0oUjUaDG5jQQxVjwBehHnp/CIw8T0YiLryDd9ukPzeavDyDzUjVt89Bf+Yo89p7T7GLzOFjjFwCY166+lHmkaSQr7oKukEVfy1e//TWppf63GWIg+24H1ezDwl0+gPghhSK4n4pdDc97XBhbARQA6YUd8E9A5CIXIqM44vDd7aLDK3KRpUJOHmskDZIVx9gWkvsyJap2Ap0HzxMO07vZDBIyxGGg1g8kBtrL4VGnTCcAqCqirfnOYyJ5HmAiUoa8qnWJGzhtti4U07WzvRkFiSTGRCfPa5QaNcqG5Oh6VFFmxX3RhQuJrgh6Zz/ib/fHTEWj+Zk2kR3+HBfFEcARaTLRKuAqsCCe1sVRDodH9Hab2S8nHuznysHnVRYHAKVOQC9w7g569tzx6tXqSquudb3QDKKpS81QaZw25msjDGy2t319ZGUScb7KV0/RE31skPD4ZDvPvgjuNy8w/Be2RU4W4s7om5SIDvIgaDjz3F5wmgp7PbauHcNlHH40yEhVyxhGkSPRINEitXD8XMTK/bofZgL7TKqiGww+tdixsFzQXPLmM2vVbbtMVKsfV3C0Th06EWsloc3BBBv317jzqlllJ763WlIf6j1eAReBf3EJkuBpL/LjHwItjXqDWZlUNZtDq6lWE5ezTcfrpEJFT4IGMS8aEXcs18yk/qrjdxbiQMDFsdvQqJGv8eoWvYC1Gan5sGUJ5u+my/FVvY4ZAgCTQf9GNNNOrmwcjr8ieNWzS2BhnQwkxhDA5uLL28c2jD5um/raRbCoLf4wrCEWlofEMiWrMEZF/vvcxN3ELDxuG5r/TbJI6rsPyKY20ZspjITX90ezqOAiEheVRmm2EJGY01ljB+yqlAktMGGu1Q8oKNp646QHJE2BFGARwzwdBukt+SVnlIESxsQ7U7SvcYAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":1034,"cache_read_input_tokens":17483,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1034},"output_tokens":8,"service_tier":"standard"}},"requestId":"req_011CXgd5tkYifNtZfvcNCc6t","type":"assistant","uuid":"1f19089c-9c61-4b9a-a5ee-45df91fbd84a","timestamp":"2026-02-01T03:16:10.183Z"}
{"parentUuid":"1f19089c-9c61-4b9a-a5ee-45df91fbd84a","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","message":{"model":"claude-opus-4-5-20251101","id":"msg_017R7UEKDjQkrsHdjSZBxV7X","type":"message","role":"assistant","content":[{"type":"text","text":"I'll build a comprehensive Prompt Engineering toolkit for PromptWizz. Let me start by checking the project structure and then create the files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":1034,"cache_read_input_tokens":17483,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1034},"output_tokens":8,"service_tier":"standard"}},"requestId":"req_011CXgd5tkYifNtZfvcNCc6t","type":"assistant","uuid":"7c13695a-f2ad-401f-8268-8626ed479196","timestamp":"2026-02-01T03:16:10.734Z"}
{"parentUuid":"7c13695a-f2ad-401f-8268-8626ed479196","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","message":{"model":"claude-opus-4-5-20251101","id":"msg_017R7UEKDjQkrsHdjSZBxV7X","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016sPhpu9AtpQkTbucsDzFW1","name":"Bash","input":{"command":"ls -la /workspace/projects/promptwizz-tools","description":"List current project directory contents"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":1034,"cache_read_input_tokens":17483,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1034},"output_tokens":8,"service_tier":"standard"}},"requestId":"req_011CXgd5tkYifNtZfvcNCc6t","type":"assistant","uuid":"ac9743cc-833d-44e5-943d-c2c2497a25bc","timestamp":"2026-02-01T03:16:11.424Z"}
{"parentUuid":"ac9743cc-833d-44e5-943d-c2c2497a25bc","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016sPhpu9AtpQkTbucsDzFW1","type":"tool_result","content":"total 3913\ndrwxrwxrwx  3 root root 1002331 Feb  1 03:15 .\ndrwxrwxrwx 15 root root 2001689 Feb  1 03:15 ..\ndrwxrwxrwx  7 root root 1002331 Feb  1 03:16 .git","is_error":false}]},"uuid":"15f12c99-be42-4de1-9deb-44d757776c00","timestamp":"2026-02-01T03:16:11.723Z","toolUseResult":{"stdout":"total 3913\ndrwxrwxrwx  3 root root 1002331 Feb  1 03:15 .\ndrwxrwxrwx 15 root root 2001689 Feb  1 03:15 ..\ndrwxrwxrwx  7 root root 1002331 Feb  1 03:16 .git","stderr":"","interrupted":false,"isImage":false},"sourceToolAssistantUUID":"ac9743cc-833d-44e5-943d-c2c2497a25bc"}
{"parentUuid":"15f12c99-be42-4de1-9deb-44d757776c00","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The directory is empty except for .git. I'll create the toolkit from scratch. Let me create all the files now.","signature":"EpYCCkYICxgCKkDW95Uf2f7GIIvoraHxSVWRYFykbxJXg3YxGQzOEFTLr0Sw+TAc3HTtKg9CoW5Wa6zyH6eEFiwlBQGFqP4eMjS4EgxM/GYKB2DicKVoKpkaDOwEE8NsIAou+RCLqCIwO/HqVVZoGvRVi++aRqHLdLZmlDviVBt7l0vaNFJcx0ir7E33XLUaE88hn+ESozSKKn57hEdPw6KuwcDXx6/8NMBZCqpsgLRn61dMYQAZYfnRTC9EolflAVs9ktJBe7F1uzvqXzt+L7N2QaFIONvz1wXbCXX9Fwvpb/DBsNf8ukMPqiGzolAXnEHwVW/GHVlGlG+xOcRvNP97mxChkzLz7KBm6be7hK2d7HETC8r0dwYYAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"e3f66ef8-7cb3-484f-bc7a-83876a4c2a59","timestamp":"2026-02-01T03:16:15.954Z"}
{"parentUuid":"e3f66ef8-7cb3-484f-bc7a-83876a4c2a59","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"text","text":"I'll create the complete toolkit with all four components."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"8a76b1e6-abb7-45f1-b3d4-c9dd20f256cd","timestamp":"2026-02-01T03:16:16.533Z"}
{"type":"file-history-snapshot","messageId":"0067784d-efd2-4b08-bb7b-b2fc68b1b32f","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"8a76b1e6-abb7-45f1-b3d4-c9dd20f256cd","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GDBvCbeLzKMVGQB6KLMVNm","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/analyzer.py","content":"\"\"\"\nPromptWizz Prompt Analyzer\n\nAnalyzes prompts for clarity, specificity, and structure.\nIdentifies weaknesses and suggests improvements.\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Optional\n\nimport anthropic\n\n\nclass ScoreCategory(Enum):\n    \"\"\"Categories for prompt scoring.\"\"\"\n    CLARITY = \"clarity\"\n    SPECIFICITY = \"specificity\"\n    STRUCTURE = \"structure\"\n    CONTEXT = \"context\"\n    ACTIONABILITY = \"actionability\"\n\n\n@dataclass\nclass Weakness:\n    \"\"\"Represents an identified weakness in a prompt.\"\"\"\n    category: str\n    description: str\n    location: Optional[str] = None\n    severity: str = \"medium\"  # low, medium, high\n\n\n@dataclass\nclass Improvement:\n    \"\"\"Suggested improvement for a prompt.\"\"\"\n    category: str\n    suggestion: str\n    example: Optional[str] = None\n    priority: int = 1  # 1 = highest priority\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Complete analysis result for a prompt.\"\"\"\n    original_prompt: str\n    scores: dict[str, float] = field(default_factory=dict)\n    overall_score: float = 0.0\n    weaknesses: list[Weakness] = field(default_factory=list)\n    improvements: list[Improvement] = field(default_factory=list)\n    improved_prompt: Optional[str] = None\n\n    def summary(self) -> str:\n        \"\"\"Generate a human-readable summary of the analysis.\"\"\"\n        lines = [\n            \"=\" * 60,\n            \"PROMPTWIZZ ANALYSIS REPORT\",\n            \"=\" * 60,\n            \"\",\n            f\"Overall Score: {self.overall_score:.1f}/100\",\n            \"\",\n            \"Category Scores:\",\n        ]\n        for category, score in self.scores.items():\n            bar = \"█\" * int(score / 10) + \"░\" * (10 - int(score / 10))\n            lines.append(f\"  {category.capitalize():15} [{bar}] {score:.1f}\")\n\n        if self.weaknesses:\n            lines.extend([\"\", \"Weaknesses Found:\"])\n            for w in self.weaknesses:\n                lines.append(f\"  • [{w.severity.upper()}] {w.category}: {w.description}\")\n\n        if self.improvements:\n            lines.extend([\"\", \"Suggested Improvements:\"])\n            for imp in sorted(self.improvements, key=lambda x: x.priority):\n                lines.append(f\"  {imp.priority}. {imp.suggestion}\")\n                if imp.example:\n                    lines.append(f\"     Example: {imp.example}\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 60)\n        return \"\\n\".join(lines)\n\n\nclass PromptAnalyzer:\n    \"\"\"\n    Analyzes prompts for quality and provides improvement suggestions.\n\n    Uses both rule-based analysis and Claude AI for deep analysis.\n    \"\"\"\n\n    # Vague terms that often indicate weak prompts\n    VAGUE_TERMS = [\n        \"good\", \"better\", \"best\", \"nice\", \"great\", \"interesting\",\n        \"some\", \"something\", \"stuff\", \"things\", \"etc\", \"whatever\",\n        \"maybe\", \"probably\", \"kind of\", \"sort of\", \"like\",\n        \"help me\", \"can you\", \"i want\", \"i need\"\n    ]\n\n    # Structural markers that indicate well-organized prompts\n    STRUCTURE_MARKERS = [\n        \"step\", \"first\", \"second\", \"then\", \"finally\", \"next\",\n        \"1.\", \"2.\", \"3.\", \"-\", \"•\", \"*\",\n        \"input:\", \"output:\", \"example:\", \"context:\", \"goal:\",\n        \"requirements:\", \"constraints:\", \"format:\"\n    ]\n\n    # Context indicators\n    CONTEXT_INDICATORS = [\n        \"background\", \"context\", \"situation\", \"scenario\",\n        \"given\", \"assuming\", \"based on\", \"considering\",\n        \"role\", \"persona\", \"act as\", \"you are\"\n    ]\n\n    def __init__(self, use_ai: bool = True, api_key: Optional[str] = None):\n        \"\"\"\n        Initialize the analyzer.\n\n        Args:\n            use_ai: Whether to use Claude AI for deep analysis\n            api_key: Anthropic API key (uses ANTHROPIC_API_KEY env var if not provided)\n        \"\"\"\n        self.use_ai = use_ai\n        self.client = None\n        if use_ai:\n            self.client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n\n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"\n        Perform comprehensive analysis of a prompt.\n\n        Args:\n            prompt: The prompt text to analyze\n\n        Returns:\n            AnalysisResult with scores, weaknesses, and improvements\n        \"\"\"\n        result = AnalysisResult(original_prompt=prompt)\n\n        # Rule-based analysis\n        result.scores[\"clarity\"] = self._score_clarity(prompt)\n        result.scores[\"specificity\"] = self._score_specificity(prompt)\n        result.scores[\"structure\"] = self._score_structure(prompt)\n        result.scores[\"context\"] = self._score_context(prompt)\n        result.scores[\"actionability\"] = self._score_actionability(prompt)\n\n        # Calculate overall score (weighted average)\n        weights = {\n            \"clarity\": 0.25,\n            \"specificity\": 0.25,\n            \"structure\": 0.20,\n            \"context\": 0.15,\n            \"actionability\": 0.15\n        }\n        result.overall_score = sum(\n            result.scores[cat] * weight\n            for cat, weight in weights.items()\n        )\n\n        # Identify weaknesses\n        result.weaknesses = self._identify_weaknesses(prompt, result.scores)\n\n        # Generate improvements\n        result.improvements = self._generate_improvements(prompt, result.weaknesses)\n\n        # Use AI for enhanced analysis if enabled\n        if self.use_ai and self.client:\n            ai_analysis = self._ai_analysis(prompt)\n            if ai_analysis:\n                result.improved_prompt = ai_analysis.get(\"improved_prompt\")\n                # Merge AI suggestions with rule-based ones\n                for suggestion in ai_analysis.get(\"suggestions\", []):\n                    result.improvements.append(Improvement(\n                        category=\"ai_suggestion\",\n                        suggestion=suggestion,\n                        priority=2\n                    ))\n\n        return result\n\n    def _score_clarity(self, prompt: str) -> float:\n        \"\"\"Score prompt clarity (0-100).\"\"\"\n        score = 70.0  # Base score\n\n        # Penalize vague terms\n        prompt_lower = prompt.lower()\n        vague_count = sum(1 for term in self.VAGUE_TERMS if term in prompt_lower)\n        score -= vague_count * 5\n\n        # Penalize very short prompts\n        word_count = len(prompt.split())\n        if word_count < 5:\n            score -= 20\n        elif word_count < 10:\n            score -= 10\n\n        # Penalize run-on sentences (no punctuation)\n        if len(prompt) > 100 and prompt.count('.') + prompt.count('?') + prompt.count('!') < 2:\n            score -= 15\n\n        # Reward clear sentence structure\n        sentences = re.split(r'[.!?]', prompt)\n        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)\n        if 10 <= avg_sentence_length <= 25:\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_specificity(self, prompt: str) -> float:\n        \"\"\"Score prompt specificity (0-100).\"\"\"\n        score = 60.0\n\n        # Reward specific quantities/numbers\n        if re.search(r'\\b\\d+\\b', prompt):\n            score += 10\n\n        # Reward specific formats mentioned\n        format_terms = [\"json\", \"csv\", \"markdown\", \"html\", \"xml\", \"bullet\", \"table\", \"list\"]\n        if any(term in prompt.lower() for term in format_terms):\n            score += 15\n\n        # Reward length specifications\n        if re.search(r'\\b(word|character|sentence|paragraph|line)s?\\b', prompt.lower()):\n            score += 10\n\n        # Reward technical terms (indicates domain specificity)\n        if re.search(r'[A-Z][a-z]+(?:[A-Z][a-z]+)+', prompt):  # CamelCase\n            score += 5\n\n        # Penalize open-ended requests\n        open_ended = [\"anything\", \"whatever you want\", \"up to you\", \"your choice\"]\n        if any(term in prompt.lower() for term in open_ended):\n            score -= 20\n\n        return max(0, min(100, score))\n\n    def _score_structure(self, prompt: str) -> float:\n        \"\"\"Score prompt structure (0-100).\"\"\"\n        score = 50.0\n\n        # Reward structural markers\n        prompt_lower = prompt.lower()\n        structure_count = sum(1 for marker in self.STRUCTURE_MARKERS if marker in prompt_lower)\n        score += min(structure_count * 8, 30)\n\n        # Reward line breaks (indicates organization)\n        if '\\n' in prompt:\n            score += 10\n\n        # Reward section headers\n        if re.search(r'^#+\\s|\\*\\*\\w+\\*\\*|__\\w+__|^\\w+:', prompt, re.MULTILINE):\n            score += 15\n\n        # Reward examples\n        if \"example\" in prompt_lower or \"e.g.\" in prompt_lower or \"for instance\" in prompt_lower:\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_context(self, prompt: str) -> float:\n        \"\"\"Score context provision (0-100).\"\"\"\n        score = 40.0\n\n        prompt_lower = prompt.lower()\n\n        # Reward context indicators\n        context_count = sum(1 for ind in self.CONTEXT_INDICATORS if ind in prompt_lower)\n        score += min(context_count * 10, 40)\n\n        # Reward role/persona assignment\n        if re.search(r'(you are|act as|pretend|imagine|role)', prompt_lower):\n            score += 15\n\n        # Reward audience specification\n        if re.search(r'(for|audience|reader|user|beginner|expert|professional)', prompt_lower):\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_actionability(self, prompt: str) -> float:\n        \"\"\"Score how actionable the prompt is (0-100).\"\"\"\n        score = 50.0\n\n        prompt_lower = prompt.lower()\n\n        # Reward action verbs at start\n        action_verbs = [\n            \"write\", \"create\", \"generate\", \"explain\", \"analyze\", \"compare\",\n            \"list\", \"describe\", \"summarize\", \"translate\", \"convert\", \"fix\",\n            \"review\", \"optimize\", \"design\", \"implement\", \"calculate\"\n        ]\n        first_word = prompt_lower.split()[0] if prompt.split() else \"\"\n        if first_word in action_verbs:\n            score += 20\n        elif any(verb in prompt_lower for verb in action_verbs):\n            score += 10\n\n        # Reward clear output specification\n        output_specs = [\"output\", \"result\", \"return\", \"provide\", \"give me\", \"show\"]\n        if any(spec in prompt_lower for spec in output_specs):\n            score += 15\n\n        # Penalize questions without clear ask\n        if prompt.strip().endswith('?') and len(prompt) < 50:\n            score -= 10\n\n        return max(0, min(100, score))\n\n    def _identify_weaknesses(self, prompt: str, scores: dict) -> list[Weakness]:\n        \"\"\"Identify specific weaknesses in the prompt.\"\"\"\n        weaknesses = []\n        prompt_lower = prompt.lower()\n\n        # Check for vague terms\n        found_vague = [term for term in self.VAGUE_TERMS if term in prompt_lower]\n        if found_vague:\n            weaknesses.append(Weakness(\n                category=\"Vague Language\",\n                description=f\"Contains vague terms: {', '.join(found_vague[:5])}\",\n                severity=\"medium\"\n            ))\n\n        # Check for missing context\n        if scores.get(\"context\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Missing Context\",\n                description=\"Prompt lacks background information or role assignment\",\n                severity=\"high\"\n            ))\n\n        # Check for poor structure\n        if scores.get(\"structure\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Poor Structure\",\n                description=\"Prompt lacks clear organization or formatting\",\n                severity=\"medium\"\n            ))\n\n        # Check for ambiguous output expectations\n        if scores.get(\"specificity\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Ambiguous Output\",\n                description=\"Output format and expectations are not clearly defined\",\n                severity=\"high\"\n            ))\n\n        # Check for too short\n        if len(prompt.split()) < 10:\n            weaknesses.append(Weakness(\n                category=\"Too Brief\",\n                description=\"Prompt may be too short to provide adequate guidance\",\n                severity=\"low\"\n            ))\n\n        return weaknesses\n\n    def _generate_improvements(self, prompt: str, weaknesses: list[Weakness]) -> list[Improvement]:\n        \"\"\"Generate improvement suggestions based on weaknesses.\"\"\"\n        improvements = []\n\n        weakness_categories = {w.category for w in weaknesses}\n\n        if \"Vague Language\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"clarity\",\n                suggestion=\"Replace vague terms with specific, measurable language\",\n                example=\"Instead of 'good code', specify 'code that follows PEP 8 style guidelines'\",\n                priority=1\n            ))\n\n        if \"Missing Context\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"context\",\n                suggestion=\"Add background context or assign a specific role/persona\",\n                example=\"Start with 'You are an expert Python developer. Given the following codebase...'\",\n                priority=1\n            ))\n\n        if \"Poor Structure\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"structure\",\n                suggestion=\"Organize prompt with clear sections (Context, Task, Format, Examples)\",\n                example=\"Use headers like '## Context:', '## Task:', '## Expected Output:'\",\n                priority=2\n            ))\n\n        if \"Ambiguous Output\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"specificity\",\n                suggestion=\"Specify exact output format, length, and style requirements\",\n                example=\"Add 'Respond in JSON format with keys: title, summary, tags'\",\n                priority=1\n            ))\n\n        if \"Too Brief\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"detail\",\n                suggestion=\"Expand prompt with more details, constraints, and examples\",\n                priority=3\n            ))\n\n        return improvements\n\n    def _ai_analysis(self, prompt: str) -> Optional[dict]:\n        \"\"\"Use Claude to provide deep analysis and rewrite.\"\"\"\n        if not self.client:\n            return None\n\n        analysis_prompt = f\"\"\"Analyze this prompt and provide improvements.\n\nPROMPT TO ANALYZE:\n{prompt}\n\nRespond in this exact format:\nIMPROVED_PROMPT:\n[Your improved version of the prompt]\n\nSUGGESTIONS:\n- [Suggestion 1]\n- [Suggestion 2]\n- [Suggestion 3]\"\"\"\n\n        try:\n            response = self.client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=1024,\n                messages=[{\"role\": \"user\", \"content\": analysis_prompt}]\n            )\n\n            content = response.content[0].text\n\n            # Parse response\n            result = {\"suggestions\": []}\n\n            if \"IMPROVED_PROMPT:\" in content:\n                improved_section = content.split(\"IMPROVED_PROMPT:\")[1]\n                if \"SUGGESTIONS:\" in improved_section:\n                    result[\"improved_prompt\"] = improved_section.split(\"SUGGESTIONS:\")[0].strip()\n                else:\n                    result[\"improved_prompt\"] = improved_section.strip()\n\n            if \"SUGGESTIONS:\" in content:\n                suggestions_section = content.split(\"SUGGESTIONS:\")[1]\n                for line in suggestions_section.split('\\n'):\n                    line = line.strip()\n                    if line.startswith('-') or line.startswith('•'):\n                        result[\"suggestions\"].append(line[1:].strip())\n\n            return result\n\n        except Exception as e:\n            print(f\"AI analysis error: {e}\")\n            return None\n\n    def quick_score(self, prompt: str) -> float:\n        \"\"\"\n        Get a quick overall score without full analysis.\n\n        Args:\n            prompt: The prompt to score\n\n        Returns:\n            Score from 0-100\n        \"\"\"\n        scores = {\n            \"clarity\": self._score_clarity(prompt),\n            \"specificity\": self._score_specificity(prompt),\n            \"structure\": self._score_structure(prompt),\n            \"context\": self._score_context(prompt),\n            \"actionability\": self._score_actionability(prompt)\n        }\n        weights = {\"clarity\": 0.25, \"specificity\": 0.25, \"structure\": 0.20,\n                   \"context\": 0.15, \"actionability\": 0.15}\n        return sum(scores[cat] * weight for cat, weight in weights.items())\n\n\ndef analyze_prompt(prompt: str, use_ai: bool = True) -> AnalysisResult:\n    \"\"\"\n    Convenience function to analyze a prompt.\n\n    Args:\n        prompt: The prompt text to analyze\n        use_ai: Whether to use Claude AI for enhanced analysis\n\n    Returns:\n        AnalysisResult with comprehensive analysis\n    \"\"\"\n    analyzer = PromptAnalyzer(use_ai=use_ai)\n    return analyzer.analyze(prompt)\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    test_prompt = \"Write me something good about Python\"\n\n    print(\"Analyzing prompt:\", repr(test_prompt))\n    print()\n\n    result = analyze_prompt(test_prompt, use_ai=False)\n    print(result.summary())\n\n    if result.improved_prompt:\n        print(\"\\nImproved version:\")\n        print(result.improved_prompt)\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"0067784d-efd2-4b08-bb7b-b2fc68b1b32f","timestamp":"2026-02-01T03:17:14.378Z"}
{"parentUuid":"0067784d-efd2-4b08-bb7b-b2fc68b1b32f","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01GDBvCbeLzKMVGQB6KLMVNm","toolUseID":"toolu_01GDBvCbeLzKMVGQB6KLMVNm","timestamp":"2026-02-01T03:17:14.494Z","uuid":"243aa2be-370d-45fc-b939-756b8d8c98d6"}
{"parentUuid":"0067784d-efd2-4b08-bb7b-b2fc68b1b32f","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GDBvCbeLzKMVGQB6KLMVNm","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/analyzer.py"}]},"uuid":"30ef3977-c387-4e20-a348-9d0b63f92e80","timestamp":"2026-02-01T03:17:14.490Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/analyzer.py","content":"\"\"\"\nPromptWizz Prompt Analyzer\n\nAnalyzes prompts for clarity, specificity, and structure.\nIdentifies weaknesses and suggests improvements.\n\"\"\"\n\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Optional\n\nimport anthropic\n\n\nclass ScoreCategory(Enum):\n    \"\"\"Categories for prompt scoring.\"\"\"\n    CLARITY = \"clarity\"\n    SPECIFICITY = \"specificity\"\n    STRUCTURE = \"structure\"\n    CONTEXT = \"context\"\n    ACTIONABILITY = \"actionability\"\n\n\n@dataclass\nclass Weakness:\n    \"\"\"Represents an identified weakness in a prompt.\"\"\"\n    category: str\n    description: str\n    location: Optional[str] = None\n    severity: str = \"medium\"  # low, medium, high\n\n\n@dataclass\nclass Improvement:\n    \"\"\"Suggested improvement for a prompt.\"\"\"\n    category: str\n    suggestion: str\n    example: Optional[str] = None\n    priority: int = 1  # 1 = highest priority\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Complete analysis result for a prompt.\"\"\"\n    original_prompt: str\n    scores: dict[str, float] = field(default_factory=dict)\n    overall_score: float = 0.0\n    weaknesses: list[Weakness] = field(default_factory=list)\n    improvements: list[Improvement] = field(default_factory=list)\n    improved_prompt: Optional[str] = None\n\n    def summary(self) -> str:\n        \"\"\"Generate a human-readable summary of the analysis.\"\"\"\n        lines = [\n            \"=\" * 60,\n            \"PROMPTWIZZ ANALYSIS REPORT\",\n            \"=\" * 60,\n            \"\",\n            f\"Overall Score: {self.overall_score:.1f}/100\",\n            \"\",\n            \"Category Scores:\",\n        ]\n        for category, score in self.scores.items():\n            bar = \"█\" * int(score / 10) + \"░\" * (10 - int(score / 10))\n            lines.append(f\"  {category.capitalize():15} [{bar}] {score:.1f}\")\n\n        if self.weaknesses:\n            lines.extend([\"\", \"Weaknesses Found:\"])\n            for w in self.weaknesses:\n                lines.append(f\"  • [{w.severity.upper()}] {w.category}: {w.description}\")\n\n        if self.improvements:\n            lines.extend([\"\", \"Suggested Improvements:\"])\n            for imp in sorted(self.improvements, key=lambda x: x.priority):\n                lines.append(f\"  {imp.priority}. {imp.suggestion}\")\n                if imp.example:\n                    lines.append(f\"     Example: {imp.example}\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 60)\n        return \"\\n\".join(lines)\n\n\nclass PromptAnalyzer:\n    \"\"\"\n    Analyzes prompts for quality and provides improvement suggestions.\n\n    Uses both rule-based analysis and Claude AI for deep analysis.\n    \"\"\"\n\n    # Vague terms that often indicate weak prompts\n    VAGUE_TERMS = [\n        \"good\", \"better\", \"best\", \"nice\", \"great\", \"interesting\",\n        \"some\", \"something\", \"stuff\", \"things\", \"etc\", \"whatever\",\n        \"maybe\", \"probably\", \"kind of\", \"sort of\", \"like\",\n        \"help me\", \"can you\", \"i want\", \"i need\"\n    ]\n\n    # Structural markers that indicate well-organized prompts\n    STRUCTURE_MARKERS = [\n        \"step\", \"first\", \"second\", \"then\", \"finally\", \"next\",\n        \"1.\", \"2.\", \"3.\", \"-\", \"•\", \"*\",\n        \"input:\", \"output:\", \"example:\", \"context:\", \"goal:\",\n        \"requirements:\", \"constraints:\", \"format:\"\n    ]\n\n    # Context indicators\n    CONTEXT_INDICATORS = [\n        \"background\", \"context\", \"situation\", \"scenario\",\n        \"given\", \"assuming\", \"based on\", \"considering\",\n        \"role\", \"persona\", \"act as\", \"you are\"\n    ]\n\n    def __init__(self, use_ai: bool = True, api_key: Optional[str] = None):\n        \"\"\"\n        Initialize the analyzer.\n\n        Args:\n            use_ai: Whether to use Claude AI for deep analysis\n            api_key: Anthropic API key (uses ANTHROPIC_API_KEY env var if not provided)\n        \"\"\"\n        self.use_ai = use_ai\n        self.client = None\n        if use_ai:\n            self.client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n\n    def analyze(self, prompt: str) -> AnalysisResult:\n        \"\"\"\n        Perform comprehensive analysis of a prompt.\n\n        Args:\n            prompt: The prompt text to analyze\n\n        Returns:\n            AnalysisResult with scores, weaknesses, and improvements\n        \"\"\"\n        result = AnalysisResult(original_prompt=prompt)\n\n        # Rule-based analysis\n        result.scores[\"clarity\"] = self._score_clarity(prompt)\n        result.scores[\"specificity\"] = self._score_specificity(prompt)\n        result.scores[\"structure\"] = self._score_structure(prompt)\n        result.scores[\"context\"] = self._score_context(prompt)\n        result.scores[\"actionability\"] = self._score_actionability(prompt)\n\n        # Calculate overall score (weighted average)\n        weights = {\n            \"clarity\": 0.25,\n            \"specificity\": 0.25,\n            \"structure\": 0.20,\n            \"context\": 0.15,\n            \"actionability\": 0.15\n        }\n        result.overall_score = sum(\n            result.scores[cat] * weight\n            for cat, weight in weights.items()\n        )\n\n        # Identify weaknesses\n        result.weaknesses = self._identify_weaknesses(prompt, result.scores)\n\n        # Generate improvements\n        result.improvements = self._generate_improvements(prompt, result.weaknesses)\n\n        # Use AI for enhanced analysis if enabled\n        if self.use_ai and self.client:\n            ai_analysis = self._ai_analysis(prompt)\n            if ai_analysis:\n                result.improved_prompt = ai_analysis.get(\"improved_prompt\")\n                # Merge AI suggestions with rule-based ones\n                for suggestion in ai_analysis.get(\"suggestions\", []):\n                    result.improvements.append(Improvement(\n                        category=\"ai_suggestion\",\n                        suggestion=suggestion,\n                        priority=2\n                    ))\n\n        return result\n\n    def _score_clarity(self, prompt: str) -> float:\n        \"\"\"Score prompt clarity (0-100).\"\"\"\n        score = 70.0  # Base score\n\n        # Penalize vague terms\n        prompt_lower = prompt.lower()\n        vague_count = sum(1 for term in self.VAGUE_TERMS if term in prompt_lower)\n        score -= vague_count * 5\n\n        # Penalize very short prompts\n        word_count = len(prompt.split())\n        if word_count < 5:\n            score -= 20\n        elif word_count < 10:\n            score -= 10\n\n        # Penalize run-on sentences (no punctuation)\n        if len(prompt) > 100 and prompt.count('.') + prompt.count('?') + prompt.count('!') < 2:\n            score -= 15\n\n        # Reward clear sentence structure\n        sentences = re.split(r'[.!?]', prompt)\n        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)\n        if 10 <= avg_sentence_length <= 25:\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_specificity(self, prompt: str) -> float:\n        \"\"\"Score prompt specificity (0-100).\"\"\"\n        score = 60.0\n\n        # Reward specific quantities/numbers\n        if re.search(r'\\b\\d+\\b', prompt):\n            score += 10\n\n        # Reward specific formats mentioned\n        format_terms = [\"json\", \"csv\", \"markdown\", \"html\", \"xml\", \"bullet\", \"table\", \"list\"]\n        if any(term in prompt.lower() for term in format_terms):\n            score += 15\n\n        # Reward length specifications\n        if re.search(r'\\b(word|character|sentence|paragraph|line)s?\\b', prompt.lower()):\n            score += 10\n\n        # Reward technical terms (indicates domain specificity)\n        if re.search(r'[A-Z][a-z]+(?:[A-Z][a-z]+)+', prompt):  # CamelCase\n            score += 5\n\n        # Penalize open-ended requests\n        open_ended = [\"anything\", \"whatever you want\", \"up to you\", \"your choice\"]\n        if any(term in prompt.lower() for term in open_ended):\n            score -= 20\n\n        return max(0, min(100, score))\n\n    def _score_structure(self, prompt: str) -> float:\n        \"\"\"Score prompt structure (0-100).\"\"\"\n        score = 50.0\n\n        # Reward structural markers\n        prompt_lower = prompt.lower()\n        structure_count = sum(1 for marker in self.STRUCTURE_MARKERS if marker in prompt_lower)\n        score += min(structure_count * 8, 30)\n\n        # Reward line breaks (indicates organization)\n        if '\\n' in prompt:\n            score += 10\n\n        # Reward section headers\n        if re.search(r'^#+\\s|\\*\\*\\w+\\*\\*|__\\w+__|^\\w+:', prompt, re.MULTILINE):\n            score += 15\n\n        # Reward examples\n        if \"example\" in prompt_lower or \"e.g.\" in prompt_lower or \"for instance\" in prompt_lower:\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_context(self, prompt: str) -> float:\n        \"\"\"Score context provision (0-100).\"\"\"\n        score = 40.0\n\n        prompt_lower = prompt.lower()\n\n        # Reward context indicators\n        context_count = sum(1 for ind in self.CONTEXT_INDICATORS if ind in prompt_lower)\n        score += min(context_count * 10, 40)\n\n        # Reward role/persona assignment\n        if re.search(r'(you are|act as|pretend|imagine|role)', prompt_lower):\n            score += 15\n\n        # Reward audience specification\n        if re.search(r'(for|audience|reader|user|beginner|expert|professional)', prompt_lower):\n            score += 10\n\n        return max(0, min(100, score))\n\n    def _score_actionability(self, prompt: str) -> float:\n        \"\"\"Score how actionable the prompt is (0-100).\"\"\"\n        score = 50.0\n\n        prompt_lower = prompt.lower()\n\n        # Reward action verbs at start\n        action_verbs = [\n            \"write\", \"create\", \"generate\", \"explain\", \"analyze\", \"compare\",\n            \"list\", \"describe\", \"summarize\", \"translate\", \"convert\", \"fix\",\n            \"review\", \"optimize\", \"design\", \"implement\", \"calculate\"\n        ]\n        first_word = prompt_lower.split()[0] if prompt.split() else \"\"\n        if first_word in action_verbs:\n            score += 20\n        elif any(verb in prompt_lower for verb in action_verbs):\n            score += 10\n\n        # Reward clear output specification\n        output_specs = [\"output\", \"result\", \"return\", \"provide\", \"give me\", \"show\"]\n        if any(spec in prompt_lower for spec in output_specs):\n            score += 15\n\n        # Penalize questions without clear ask\n        if prompt.strip().endswith('?') and len(prompt) < 50:\n            score -= 10\n\n        return max(0, min(100, score))\n\n    def _identify_weaknesses(self, prompt: str, scores: dict) -> list[Weakness]:\n        \"\"\"Identify specific weaknesses in the prompt.\"\"\"\n        weaknesses = []\n        prompt_lower = prompt.lower()\n\n        # Check for vague terms\n        found_vague = [term for term in self.VAGUE_TERMS if term in prompt_lower]\n        if found_vague:\n            weaknesses.append(Weakness(\n                category=\"Vague Language\",\n                description=f\"Contains vague terms: {', '.join(found_vague[:5])}\",\n                severity=\"medium\"\n            ))\n\n        # Check for missing context\n        if scores.get(\"context\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Missing Context\",\n                description=\"Prompt lacks background information or role assignment\",\n                severity=\"high\"\n            ))\n\n        # Check for poor structure\n        if scores.get(\"structure\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Poor Structure\",\n                description=\"Prompt lacks clear organization or formatting\",\n                severity=\"medium\"\n            ))\n\n        # Check for ambiguous output expectations\n        if scores.get(\"specificity\", 0) < 50:\n            weaknesses.append(Weakness(\n                category=\"Ambiguous Output\",\n                description=\"Output format and expectations are not clearly defined\",\n                severity=\"high\"\n            ))\n\n        # Check for too short\n        if len(prompt.split()) < 10:\n            weaknesses.append(Weakness(\n                category=\"Too Brief\",\n                description=\"Prompt may be too short to provide adequate guidance\",\n                severity=\"low\"\n            ))\n\n        return weaknesses\n\n    def _generate_improvements(self, prompt: str, weaknesses: list[Weakness]) -> list[Improvement]:\n        \"\"\"Generate improvement suggestions based on weaknesses.\"\"\"\n        improvements = []\n\n        weakness_categories = {w.category for w in weaknesses}\n\n        if \"Vague Language\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"clarity\",\n                suggestion=\"Replace vague terms with specific, measurable language\",\n                example=\"Instead of 'good code', specify 'code that follows PEP 8 style guidelines'\",\n                priority=1\n            ))\n\n        if \"Missing Context\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"context\",\n                suggestion=\"Add background context or assign a specific role/persona\",\n                example=\"Start with 'You are an expert Python developer. Given the following codebase...'\",\n                priority=1\n            ))\n\n        if \"Poor Structure\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"structure\",\n                suggestion=\"Organize prompt with clear sections (Context, Task, Format, Examples)\",\n                example=\"Use headers like '## Context:', '## Task:', '## Expected Output:'\",\n                priority=2\n            ))\n\n        if \"Ambiguous Output\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"specificity\",\n                suggestion=\"Specify exact output format, length, and style requirements\",\n                example=\"Add 'Respond in JSON format with keys: title, summary, tags'\",\n                priority=1\n            ))\n\n        if \"Too Brief\" in weakness_categories:\n            improvements.append(Improvement(\n                category=\"detail\",\n                suggestion=\"Expand prompt with more details, constraints, and examples\",\n                priority=3\n            ))\n\n        return improvements\n\n    def _ai_analysis(self, prompt: str) -> Optional[dict]:\n        \"\"\"Use Claude to provide deep analysis and rewrite.\"\"\"\n        if not self.client:\n            return None\n\n        analysis_prompt = f\"\"\"Analyze this prompt and provide improvements.\n\nPROMPT TO ANALYZE:\n{prompt}\n\nRespond in this exact format:\nIMPROVED_PROMPT:\n[Your improved version of the prompt]\n\nSUGGESTIONS:\n- [Suggestion 1]\n- [Suggestion 2]\n- [Suggestion 3]\"\"\"\n\n        try:\n            response = self.client.messages.create(\n                model=\"claude-sonnet-4-20250514\",\n                max_tokens=1024,\n                messages=[{\"role\": \"user\", \"content\": analysis_prompt}]\n            )\n\n            content = response.content[0].text\n\n            # Parse response\n            result = {\"suggestions\": []}\n\n            if \"IMPROVED_PROMPT:\" in content:\n                improved_section = content.split(\"IMPROVED_PROMPT:\")[1]\n                if \"SUGGESTIONS:\" in improved_section:\n                    result[\"improved_prompt\"] = improved_section.split(\"SUGGESTIONS:\")[0].strip()\n                else:\n                    result[\"improved_prompt\"] = improved_section.strip()\n\n            if \"SUGGESTIONS:\" in content:\n                suggestions_section = content.split(\"SUGGESTIONS:\")[1]\n                for line in suggestions_section.split('\\n'):\n                    line = line.strip()\n                    if line.startswith('-') or line.startswith('•'):\n                        result[\"suggestions\"].append(line[1:].strip())\n\n            return result\n\n        except Exception as e:\n            print(f\"AI analysis error: {e}\")\n            return None\n\n    def quick_score(self, prompt: str) -> float:\n        \"\"\"\n        Get a quick overall score without full analysis.\n\n        Args:\n            prompt: The prompt to score\n\n        Returns:\n            Score from 0-100\n        \"\"\"\n        scores = {\n            \"clarity\": self._score_clarity(prompt),\n            \"specificity\": self._score_specificity(prompt),\n            \"structure\": self._score_structure(prompt),\n            \"context\": self._score_context(prompt),\n            \"actionability\": self._score_actionability(prompt)\n        }\n        weights = {\"clarity\": 0.25, \"specificity\": 0.25, \"structure\": 0.20,\n                   \"context\": 0.15, \"actionability\": 0.15}\n        return sum(scores[cat] * weight for cat, weight in weights.items())\n\n\ndef analyze_prompt(prompt: str, use_ai: bool = True) -> AnalysisResult:\n    \"\"\"\n    Convenience function to analyze a prompt.\n\n    Args:\n        prompt: The prompt text to analyze\n        use_ai: Whether to use Claude AI for enhanced analysis\n\n    Returns:\n        AnalysisResult with comprehensive analysis\n    \"\"\"\n    analyzer = PromptAnalyzer(use_ai=use_ai)\n    return analyzer.analyze(prompt)\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    test_prompt = \"Write me something good about Python\"\n\n    print(\"Analyzing prompt:\", repr(test_prompt))\n    print()\n\n    result = analyze_prompt(test_prompt, use_ai=False)\n    print(result.summary())\n\n    if result.improved_prompt:\n        print(\"\\nImproved version:\")\n        print(result.improved_prompt)\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"0067784d-efd2-4b08-bb7b-b2fc68b1b32f"}
{"type":"file-history-snapshot","messageId":"447649e7-0fbf-4605-9a3b-45583bb3e805","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"30ef3977-c387-4e20-a348-9d0b63f92e80","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0128Buz2PTxuwCoG6cRQYK4S","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/templates.py","content":"\"\"\"\nPromptWizz Prompt Templates\n\nLibrary of optimized prompt templates with variable substitution.\nCategories: coding, writing, analysis, creative.\n\"\"\"\n\nimport json\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Optional\n\n\nclass TemplateCategory(Enum):\n    \"\"\"Categories for prompt templates.\"\"\"\n    CODING = \"coding\"\n    WRITING = \"writing\"\n    ANALYSIS = \"analysis\"\n    CREATIVE = \"creative\"\n    GENERAL = \"general\"\n\n\n@dataclass\nclass PromptTemplate:\n    \"\"\"\n    A reusable prompt template with variable substitution.\n\n    Variables are denoted with {{variable_name}} syntax.\n    Optional variables use {{variable_name?}} or {{variable_name?default_value}}.\n    \"\"\"\n    name: str\n    template: str\n    category: TemplateCategory\n    description: str = \"\"\n    variables: list[str] = field(default_factory=list)\n    optional_variables: dict[str, str] = field(default_factory=dict)  # var -> default\n    tags: list[str] = field(default_factory=list)\n    examples: list[dict[str, str]] = field(default_factory=list)\n\n    def __post_init__(self):\n        \"\"\"Extract variables from template if not provided.\"\"\"\n        if not self.variables:\n            self.variables = self._extract_variables()\n        if not self.optional_variables:\n            self.optional_variables = self._extract_optional_variables()\n\n    def _extract_variables(self) -> list[str]:\n        \"\"\"Extract required variable names from template.\"\"\"\n        # Match {{var}} but not {{var?}} or {{var?default}}\n        pattern = r'\\{\\{(\\w+)\\}\\}'\n        return list(set(re.findall(pattern, self.template)))\n\n    def _extract_optional_variables(self) -> dict[str, str]:\n        \"\"\"Extract optional variables and their defaults.\"\"\"\n        # Match {{var?}} or {{var?default}}\n        pattern = r'\\{\\{(\\w+)\\?([^}]*)\\}\\}'\n        matches = re.findall(pattern, self.template)\n        return {var: default for var, default in matches}\n\n    def render(self, **kwargs: Any) -> str:\n        \"\"\"\n        Render the template with provided variables.\n\n        Args:\n            **kwargs: Variable values to substitute\n\n        Returns:\n            Rendered prompt string\n\n        Raises:\n            ValueError: If required variables are missing\n        \"\"\"\n        result = self.template\n\n        # Check for missing required variables\n        missing = [v for v in self.variables if v not in kwargs]\n        if missing:\n            raise ValueError(f\"Missing required variables: {', '.join(missing)}\")\n\n        # Substitute required variables\n        for var, value in kwargs.items():\n            result = result.replace(f\"{{{{{var}}}}}\", str(value))\n\n        # Handle optional variables\n        for var, default in self.optional_variables.items():\n            if var in kwargs:\n                result = re.sub(\n                    rf'\\{{\\{{{var}\\?\\w*\\}}\\}}',\n                    str(kwargs[var]),\n                    result\n                )\n            else:\n                result = re.sub(\n                    rf'\\{{\\{{{var}\\?[^}}]*\\}}\\}}',\n                    default,\n                    result\n                )\n\n        return result\n\n    def get_required_variables(self) -> list[str]:\n        \"\"\"Get list of required variable names.\"\"\"\n        return self.variables.copy()\n\n    def get_all_variables(self) -> list[str]:\n        \"\"\"Get all variable names (required and optional).\"\"\"\n        return self.variables + list(self.optional_variables.keys())\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert template to dictionary for serialization.\"\"\"\n        return {\n            \"name\": self.name,\n            \"template\": self.template,\n            \"category\": self.category.value,\n            \"description\": self.description,\n            \"variables\": self.variables,\n            \"optional_variables\": self.optional_variables,\n            \"tags\": self.tags,\n            \"examples\": self.examples\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"PromptTemplate\":\n        \"\"\"Create template from dictionary.\"\"\"\n        return cls(\n            name=data[\"name\"],\n            template=data[\"template\"],\n            category=TemplateCategory(data.get(\"category\", \"general\")),\n            description=data.get(\"description\", \"\"),\n            variables=data.get(\"variables\", []),\n            optional_variables=data.get(\"optional_variables\", {}),\n            tags=data.get(\"tags\", []),\n            examples=data.get(\"examples\", [])\n        )\n\n\nclass TemplateLibrary:\n    \"\"\"\n    Library for managing and accessing prompt templates.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with built-in templates.\"\"\"\n        self.templates: dict[str, PromptTemplate] = {}\n        self._load_builtin_templates()\n\n    def _load_builtin_templates(self):\n        \"\"\"Load the built-in template library.\"\"\"\n        builtin = [\n            # CODING TEMPLATES\n            PromptTemplate(\n                name=\"code_review\",\n                category=TemplateCategory.CODING,\n                description=\"Review code for issues and improvements\",\n                template=\"\"\"You are an expert {{language}} code reviewer.\n\nReview the following code for:\n- Bugs and potential issues\n- Performance problems\n- Security vulnerabilities\n- Code style and best practices\n- Maintainability concerns\n\nCODE TO REVIEW:\n```{{language}}\n{{code}}\n```\n\n{{focus?}}\n\nProvide a structured review with:\n1. Critical issues (must fix)\n2. Recommendations (should fix)\n3. Suggestions (nice to have)\n4. Overall assessment\"\"\",\n                tags=[\"code\", \"review\", \"quality\"],\n                examples=[{\n                    \"language\": \"python\",\n                    \"code\": \"def calc(x): return x*2\",\n                    \"focus\": \"Focus on naming conventions\"\n                }]\n            ),\n\n            PromptTemplate(\n                name=\"code_explain\",\n                category=TemplateCategory.CODING,\n                description=\"Explain code in detail\",\n                template=\"\"\"Explain this {{language}} code in detail.\n\n```{{language}}\n{{code}}\n```\n\nTarget audience: {{audience?intermediate developers}}\n\nPlease cover:\n1. What the code does (high-level overview)\n2. How it works (step-by-step walkthrough)\n3. Key concepts and patterns used\n4. Potential improvements or alternatives\"\"\",\n                tags=[\"code\", \"explanation\", \"learning\"]\n            ),\n\n            PromptTemplate(\n                name=\"code_generate\",\n                category=TemplateCategory.CODING,\n                description=\"Generate code from requirements\",\n                template=\"\"\"You are an expert {{language}} developer.\n\nCreate code that accomplishes the following:\n{{requirements}}\n\nTechnical constraints:\n{{constraints?None specified}}\n\nCode style requirements:\n- Follow {{language}} best practices\n- Include clear comments for complex logic\n- Handle edge cases appropriately\n{{style?}}\n\nProvide the complete, working code with brief explanations.\"\"\",\n                tags=[\"code\", \"generation\", \"development\"]\n            ),\n\n            PromptTemplate(\n                name=\"debug_help\",\n                category=TemplateCategory.CODING,\n                description=\"Help debug code issues\",\n                template=\"\"\"I need help debugging this {{language}} code.\n\nCODE:\n```{{language}}\n{{code}}\n```\n\nERROR/ISSUE:\n{{error}}\n\nEXPECTED BEHAVIOR:\n{{expected}}\n\nPlease:\n1. Identify the root cause\n2. Explain why this error occurs\n3. Provide the corrected code\n4. Suggest how to prevent similar issues\"\"\",\n                tags=[\"code\", \"debug\", \"troubleshooting\"]\n            ),\n\n            PromptTemplate(\n                name=\"unit_test\",\n                category=TemplateCategory.CODING,\n                description=\"Generate unit tests for code\",\n                template=\"\"\"Generate comprehensive unit tests for this {{language}} code.\n\nCODE TO TEST:\n```{{language}}\n{{code}}\n```\n\nTesting framework: {{framework?pytest}}\n\nInclude tests for:\n- Normal/expected inputs\n- Edge cases\n- Error conditions\n- Boundary values\n\n{{additional_requirements?}}\n\nProvide well-organized, documented test code.\"\"\",\n                tags=[\"code\", \"testing\", \"quality\"]\n            ),\n\n            # WRITING TEMPLATES\n            PromptTemplate(\n                name=\"blog_post\",\n                category=TemplateCategory.WRITING,\n                description=\"Write a blog post on a topic\",\n                template=\"\"\"Write a blog post about: {{topic}}\n\nTarget audience: {{audience}}\nTone: {{tone?professional but approachable}}\nLength: {{length?800-1200 words}}\n\nStructure:\n- Engaging hook/introduction\n- Clear main points with examples\n- Actionable takeaways\n- Compelling conclusion\n\n{{additional_notes?}}\n\nUse subheadings, bullet points where appropriate, and maintain reader engagement throughout.\"\"\",\n                tags=[\"writing\", \"blog\", \"content\"]\n            ),\n\n            PromptTemplate(\n                name=\"email_draft\",\n                category=TemplateCategory.WRITING,\n                description=\"Draft a professional email\",\n                template=\"\"\"Draft a professional email.\n\nPURPOSE: {{purpose}}\nRECIPIENT: {{recipient}}\nTONE: {{tone?professional}}\nKEY POINTS TO COVER:\n{{key_points}}\n\n{{context?}}\n\nThe email should be clear, concise, and achieve the intended purpose while maintaining appropriate professionalism.\"\"\",\n                tags=[\"writing\", \"email\", \"communication\"]\n            ),\n\n            PromptTemplate(\n                name=\"documentation\",\n                category=TemplateCategory.WRITING,\n                description=\"Write technical documentation\",\n                template=\"\"\"Write technical documentation for: {{subject}}\n\nDocumentation type: {{doc_type?user guide}}\nTarget audience: {{audience?developers}}\n\nInclude:\n- Overview/introduction\n- Key concepts\n- Step-by-step instructions where applicable\n- Examples\n- Common issues/troubleshooting\n\n{{specific_sections?}}\n\nUse clear formatting with headers, code blocks, and notes as appropriate.\"\"\",\n                tags=[\"writing\", \"documentation\", \"technical\"]\n            ),\n\n            PromptTemplate(\n                name=\"summary\",\n                category=TemplateCategory.WRITING,\n                description=\"Summarize content\",\n                template=\"\"\"Summarize the following content.\n\nCONTENT:\n{{content}}\n\nSummary requirements:\n- Length: {{length?2-3 paragraphs}}\n- Focus on: {{focus?key points and main arguments}}\n- Format: {{format?prose}}\n\n{{additional_requirements?}}\"\"\",\n                tags=[\"writing\", \"summary\", \"condensing\"]\n            ),\n\n            # ANALYSIS TEMPLATES\n            PromptTemplate(\n                name=\"data_analysis\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Analyze data or information\",\n                template=\"\"\"Analyze the following data/information:\n\nDATA:\n{{data}}\n\nAnalysis goals:\n{{goals}}\n\nPlease provide:\n1. Key findings and patterns\n2. Statistical insights (if applicable)\n3. Anomalies or notable observations\n4. Recommendations based on findings\n5. Limitations of this analysis\n\n{{output_format?Present findings in a structured report format}}\"\"\",\n                tags=[\"analysis\", \"data\", \"insights\"]\n            ),\n\n            PromptTemplate(\n                name=\"compare_contrast\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Compare and contrast items\",\n                template=\"\"\"Compare and contrast the following:\n\nITEM 1: {{item1}}\nITEM 2: {{item2}}\n{{item3?}}\n\nComparison criteria:\n{{criteria?all relevant aspects}}\n\nProvide:\n1. Side-by-side comparison table\n2. Key similarities\n3. Key differences\n4. Use case recommendations (when to choose each)\n5. Overall assessment\"\"\",\n                tags=[\"analysis\", \"comparison\", \"decision\"]\n            ),\n\n            PromptTemplate(\n                name=\"problem_solve\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Structured problem solving\",\n                template=\"\"\"Help me solve this problem:\n\nPROBLEM:\n{{problem}}\n\nCONTEXT:\n{{context}}\n\nCONSTRAINTS:\n{{constraints?None specified}}\n\nPlease provide:\n1. Problem breakdown and root cause analysis\n2. Multiple potential solutions (at least 3)\n3. Pros and cons of each solution\n4. Recommended approach with justification\n5. Implementation steps for the recommended solution\"\"\",\n                tags=[\"analysis\", \"problem-solving\", \"strategy\"]\n            ),\n\n            PromptTemplate(\n                name=\"research\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Research and synthesize information\",\n                template=\"\"\"Research and provide comprehensive information about:\n\nTOPIC: {{topic}}\n\nScope: {{scope?comprehensive overview}}\n\nPlease cover:\n1. Definition and background\n2. Key concepts and components\n3. Current state/trends\n4. Important considerations\n5. Resources for further learning\n\n{{specific_questions?}}\n\nCite sources where applicable and indicate confidence levels for claims.\"\"\",\n                tags=[\"analysis\", \"research\", \"learning\"]\n            ),\n\n            # CREATIVE TEMPLATES\n            PromptTemplate(\n                name=\"brainstorm\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Generate creative ideas\",\n                template=\"\"\"Brainstorm ideas for: {{topic}}\n\nContext: {{context?}}\nConstraints: {{constraints?none}}\nNumber of ideas: {{count?10}}\n\nFor each idea, provide:\n- Brief description\n- Key benefits\n- Potential challenges\n- Feasibility rating (1-5)\n\nThink outside the box and include both conventional and unconventional approaches.\"\"\",\n                tags=[\"creative\", \"ideation\", \"brainstorming\"]\n            ),\n\n            PromptTemplate(\n                name=\"story_outline\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Create a story outline\",\n                template=\"\"\"Create a story outline.\n\nGenre: {{genre}}\nSetting: {{setting}}\nMain theme: {{theme}}\nTarget audience: {{audience?general}}\n\nInclude:\n1. Premise (1-2 sentences)\n2. Main characters with brief descriptions\n3. Three-act structure outline\n4. Key plot points and conflicts\n5. Potential endings\n\n{{additional_elements?}}\"\"\",\n                tags=[\"creative\", \"writing\", \"storytelling\"]\n            ),\n\n            PromptTemplate(\n                name=\"name_generator\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Generate names for projects, products, etc.\",\n                template=\"\"\"Generate name ideas for: {{subject}}\n\nType: {{type?product/project}}\nStyle: {{style?modern and memorable}}\nRequirements: {{requirements?}}\n\nGenerate {{count?15}} name suggestions.\n\nFor each name, provide:\n- The name\n- Brief explanation of meaning/inspiration\n- Domain availability likelihood (if applicable)\n\nInclude a mix of:\n- Descriptive names\n- Abstract/creative names\n- Compound words\n- Invented words\"\"\",\n                tags=[\"creative\", \"naming\", \"branding\"]\n            ),\n\n            PromptTemplate(\n                name=\"rewrite\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Rewrite content in a different style\",\n                template=\"\"\"Rewrite the following content.\n\nORIGINAL:\n{{content}}\n\nNEW STYLE: {{style}}\nTARGET AUDIENCE: {{audience?general}}\nTONE: {{tone?}}\n\nMaintain the core meaning while adapting to the new style.\n{{preserve?}}\"\"\",\n                tags=[\"creative\", \"writing\", \"adaptation\"]\n            ),\n        ]\n\n        for template in builtin:\n            self.templates[template.name] = template\n\n    def get(self, name: str) -> Optional[PromptTemplate]:\n        \"\"\"Get a template by name.\"\"\"\n        return self.templates.get(name)\n\n    def add(self, template: PromptTemplate) -> None:\n        \"\"\"Add a new template to the library.\"\"\"\n        self.templates[template.name] = template\n\n    def remove(self, name: str) -> bool:\n        \"\"\"Remove a template by name. Returns True if removed.\"\"\"\n        if name in self.templates:\n            del self.templates[name]\n            return True\n        return False\n\n    def list_templates(\n        self,\n        category: Optional[TemplateCategory] = None,\n        tags: Optional[list[str]] = None\n    ) -> list[PromptTemplate]:\n        \"\"\"\n        List templates, optionally filtered by category or tags.\n\n        Args:\n            category: Filter by category\n            tags: Filter by tags (any match)\n\n        Returns:\n            List of matching templates\n        \"\"\"\n        results = list(self.templates.values())\n\n        if category:\n            results = [t for t in results if t.category == category]\n\n        if tags:\n            results = [t for t in results if any(tag in t.tags for tag in tags)]\n\n        return sorted(results, key=lambda t: t.name)\n\n    def search(self, query: str) -> list[PromptTemplate]:\n        \"\"\"Search templates by name, description, or tags.\"\"\"\n        query = query.lower()\n        results = []\n\n        for template in self.templates.values():\n            if (query in template.name.lower() or\n                query in template.description.lower() or\n                any(query in tag.lower() for tag in template.tags)):\n                results.append(template)\n\n        return sorted(results, key=lambda t: t.name)\n\n    def render(self, name: str, **kwargs: Any) -> str:\n        \"\"\"\n        Render a template by name with provided variables.\n\n        Args:\n            name: Template name\n            **kwargs: Variable values\n\n        Returns:\n            Rendered prompt string\n\n        Raises:\n            KeyError: If template not found\n            ValueError: If required variables missing\n        \"\"\"\n        template = self.get(name)\n        if not template:\n            raise KeyError(f\"Template not found: {name}\")\n        return template.render(**kwargs)\n\n    def export_to_json(self, filepath: str) -> None:\n        \"\"\"Export all templates to a JSON file.\"\"\"\n        data = {name: t.to_dict() for name, t in self.templates.items()}\n        Path(filepath).write_text(json.dumps(data, indent=2))\n\n    def import_from_json(self, filepath: str) -> int:\n        \"\"\"\n        Import templates from a JSON file.\n\n        Returns:\n            Number of templates imported\n        \"\"\"\n        data = json.loads(Path(filepath).read_text())\n        count = 0\n        for name, template_data in data.items():\n            self.templates[name] = PromptTemplate.from_dict(template_data)\n            count += 1\n        return count\n\n    def get_categories(self) -> list[TemplateCategory]:\n        \"\"\"Get list of categories that have templates.\"\"\"\n        return list(set(t.category for t in self.templates.values()))\n\n    def get_all_tags(self) -> list[str]:\n        \"\"\"Get all unique tags across templates.\"\"\"\n        tags = set()\n        for template in self.templates.values():\n            tags.update(template.tags)\n        return sorted(tags)\n\n\n# Global library instance\nlibrary = TemplateLibrary()\n\n\ndef get_template(name: str) -> Optional[PromptTemplate]:\n    \"\"\"Get a template from the global library.\"\"\"\n    return library.get(name)\n\n\ndef render_template(name: str, **kwargs: Any) -> str:\n    \"\"\"Render a template from the global library.\"\"\"\n    return library.render(name, **kwargs)\n\n\ndef list_templates(category: Optional[str] = None) -> list[PromptTemplate]:\n    \"\"\"List templates, optionally filtered by category.\"\"\"\n    cat = TemplateCategory(category) if category else None\n    return library.list_templates(category=cat)\n\n\nif __name__ == \"__main__\":\n    # Demo usage\n    print(\"PromptWizz Template Library\")\n    print(\"=\" * 50)\n    print()\n\n    # List all categories\n    print(\"Categories:\")\n    for cat in library.get_categories():\n        templates = library.list_templates(category=cat)\n        print(f\"  {cat.value}: {len(templates)} templates\")\n\n    print()\n\n    # Show example template usage\n    print(\"Example - Code Review Template:\")\n    print(\"-\" * 50)\n\n    prompt = library.render(\n        \"code_review\",\n        language=\"python\",\n        code=\"\"\"def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\"\"\",\n        focus=\"Focus on performance optimization\"\n    )\n    print(prompt)\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"447649e7-0fbf-4605-9a3b-45583bb3e805","timestamp":"2026-02-01T03:18:19.768Z"}
{"parentUuid":"447649e7-0fbf-4605-9a3b-45583bb3e805","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_0128Buz2PTxuwCoG6cRQYK4S","toolUseID":"toolu_0128Buz2PTxuwCoG6cRQYK4S","timestamp":"2026-02-01T03:18:19.832Z","uuid":"a375d0f7-fc42-46f3-911a-35e0a20b9b2d"}
{"parentUuid":"447649e7-0fbf-4605-9a3b-45583bb3e805","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0128Buz2PTxuwCoG6cRQYK4S","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/templates.py"}]},"uuid":"81abb4c9-72c3-4d7b-a9f1-a9dcead9e810","timestamp":"2026-02-01T03:18:19.827Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/templates.py","content":"\"\"\"\nPromptWizz Prompt Templates\n\nLibrary of optimized prompt templates with variable substitution.\nCategories: coding, writing, analysis, creative.\n\"\"\"\n\nimport json\nimport re\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Optional\n\n\nclass TemplateCategory(Enum):\n    \"\"\"Categories for prompt templates.\"\"\"\n    CODING = \"coding\"\n    WRITING = \"writing\"\n    ANALYSIS = \"analysis\"\n    CREATIVE = \"creative\"\n    GENERAL = \"general\"\n\n\n@dataclass\nclass PromptTemplate:\n    \"\"\"\n    A reusable prompt template with variable substitution.\n\n    Variables are denoted with {{variable_name}} syntax.\n    Optional variables use {{variable_name?}} or {{variable_name?default_value}}.\n    \"\"\"\n    name: str\n    template: str\n    category: TemplateCategory\n    description: str = \"\"\n    variables: list[str] = field(default_factory=list)\n    optional_variables: dict[str, str] = field(default_factory=dict)  # var -> default\n    tags: list[str] = field(default_factory=list)\n    examples: list[dict[str, str]] = field(default_factory=list)\n\n    def __post_init__(self):\n        \"\"\"Extract variables from template if not provided.\"\"\"\n        if not self.variables:\n            self.variables = self._extract_variables()\n        if not self.optional_variables:\n            self.optional_variables = self._extract_optional_variables()\n\n    def _extract_variables(self) -> list[str]:\n        \"\"\"Extract required variable names from template.\"\"\"\n        # Match {{var}} but not {{var?}} or {{var?default}}\n        pattern = r'\\{\\{(\\w+)\\}\\}'\n        return list(set(re.findall(pattern, self.template)))\n\n    def _extract_optional_variables(self) -> dict[str, str]:\n        \"\"\"Extract optional variables and their defaults.\"\"\"\n        # Match {{var?}} or {{var?default}}\n        pattern = r'\\{\\{(\\w+)\\?([^}]*)\\}\\}'\n        matches = re.findall(pattern, self.template)\n        return {var: default for var, default in matches}\n\n    def render(self, **kwargs: Any) -> str:\n        \"\"\"\n        Render the template with provided variables.\n\n        Args:\n            **kwargs: Variable values to substitute\n\n        Returns:\n            Rendered prompt string\n\n        Raises:\n            ValueError: If required variables are missing\n        \"\"\"\n        result = self.template\n\n        # Check for missing required variables\n        missing = [v for v in self.variables if v not in kwargs]\n        if missing:\n            raise ValueError(f\"Missing required variables: {', '.join(missing)}\")\n\n        # Substitute required variables\n        for var, value in kwargs.items():\n            result = result.replace(f\"{{{{{var}}}}}\", str(value))\n\n        # Handle optional variables\n        for var, default in self.optional_variables.items():\n            if var in kwargs:\n                result = re.sub(\n                    rf'\\{{\\{{{var}\\?\\w*\\}}\\}}',\n                    str(kwargs[var]),\n                    result\n                )\n            else:\n                result = re.sub(\n                    rf'\\{{\\{{{var}\\?[^}}]*\\}}\\}}',\n                    default,\n                    result\n                )\n\n        return result\n\n    def get_required_variables(self) -> list[str]:\n        \"\"\"Get list of required variable names.\"\"\"\n        return self.variables.copy()\n\n    def get_all_variables(self) -> list[str]:\n        \"\"\"Get all variable names (required and optional).\"\"\"\n        return self.variables + list(self.optional_variables.keys())\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert template to dictionary for serialization.\"\"\"\n        return {\n            \"name\": self.name,\n            \"template\": self.template,\n            \"category\": self.category.value,\n            \"description\": self.description,\n            \"variables\": self.variables,\n            \"optional_variables\": self.optional_variables,\n            \"tags\": self.tags,\n            \"examples\": self.examples\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -> \"PromptTemplate\":\n        \"\"\"Create template from dictionary.\"\"\"\n        return cls(\n            name=data[\"name\"],\n            template=data[\"template\"],\n            category=TemplateCategory(data.get(\"category\", \"general\")),\n            description=data.get(\"description\", \"\"),\n            variables=data.get(\"variables\", []),\n            optional_variables=data.get(\"optional_variables\", {}),\n            tags=data.get(\"tags\", []),\n            examples=data.get(\"examples\", [])\n        )\n\n\nclass TemplateLibrary:\n    \"\"\"\n    Library for managing and accessing prompt templates.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with built-in templates.\"\"\"\n        self.templates: dict[str, PromptTemplate] = {}\n        self._load_builtin_templates()\n\n    def _load_builtin_templates(self):\n        \"\"\"Load the built-in template library.\"\"\"\n        builtin = [\n            # CODING TEMPLATES\n            PromptTemplate(\n                name=\"code_review\",\n                category=TemplateCategory.CODING,\n                description=\"Review code for issues and improvements\",\n                template=\"\"\"You are an expert {{language}} code reviewer.\n\nReview the following code for:\n- Bugs and potential issues\n- Performance problems\n- Security vulnerabilities\n- Code style and best practices\n- Maintainability concerns\n\nCODE TO REVIEW:\n```{{language}}\n{{code}}\n```\n\n{{focus?}}\n\nProvide a structured review with:\n1. Critical issues (must fix)\n2. Recommendations (should fix)\n3. Suggestions (nice to have)\n4. Overall assessment\"\"\",\n                tags=[\"code\", \"review\", \"quality\"],\n                examples=[{\n                    \"language\": \"python\",\n                    \"code\": \"def calc(x): return x*2\",\n                    \"focus\": \"Focus on naming conventions\"\n                }]\n            ),\n\n            PromptTemplate(\n                name=\"code_explain\",\n                category=TemplateCategory.CODING,\n                description=\"Explain code in detail\",\n                template=\"\"\"Explain this {{language}} code in detail.\n\n```{{language}}\n{{code}}\n```\n\nTarget audience: {{audience?intermediate developers}}\n\nPlease cover:\n1. What the code does (high-level overview)\n2. How it works (step-by-step walkthrough)\n3. Key concepts and patterns used\n4. Potential improvements or alternatives\"\"\",\n                tags=[\"code\", \"explanation\", \"learning\"]\n            ),\n\n            PromptTemplate(\n                name=\"code_generate\",\n                category=TemplateCategory.CODING,\n                description=\"Generate code from requirements\",\n                template=\"\"\"You are an expert {{language}} developer.\n\nCreate code that accomplishes the following:\n{{requirements}}\n\nTechnical constraints:\n{{constraints?None specified}}\n\nCode style requirements:\n- Follow {{language}} best practices\n- Include clear comments for complex logic\n- Handle edge cases appropriately\n{{style?}}\n\nProvide the complete, working code with brief explanations.\"\"\",\n                tags=[\"code\", \"generation\", \"development\"]\n            ),\n\n            PromptTemplate(\n                name=\"debug_help\",\n                category=TemplateCategory.CODING,\n                description=\"Help debug code issues\",\n                template=\"\"\"I need help debugging this {{language}} code.\n\nCODE:\n```{{language}}\n{{code}}\n```\n\nERROR/ISSUE:\n{{error}}\n\nEXPECTED BEHAVIOR:\n{{expected}}\n\nPlease:\n1. Identify the root cause\n2. Explain why this error occurs\n3. Provide the corrected code\n4. Suggest how to prevent similar issues\"\"\",\n                tags=[\"code\", \"debug\", \"troubleshooting\"]\n            ),\n\n            PromptTemplate(\n                name=\"unit_test\",\n                category=TemplateCategory.CODING,\n                description=\"Generate unit tests for code\",\n                template=\"\"\"Generate comprehensive unit tests for this {{language}} code.\n\nCODE TO TEST:\n```{{language}}\n{{code}}\n```\n\nTesting framework: {{framework?pytest}}\n\nInclude tests for:\n- Normal/expected inputs\n- Edge cases\n- Error conditions\n- Boundary values\n\n{{additional_requirements?}}\n\nProvide well-organized, documented test code.\"\"\",\n                tags=[\"code\", \"testing\", \"quality\"]\n            ),\n\n            # WRITING TEMPLATES\n            PromptTemplate(\n                name=\"blog_post\",\n                category=TemplateCategory.WRITING,\n                description=\"Write a blog post on a topic\",\n                template=\"\"\"Write a blog post about: {{topic}}\n\nTarget audience: {{audience}}\nTone: {{tone?professional but approachable}}\nLength: {{length?800-1200 words}}\n\nStructure:\n- Engaging hook/introduction\n- Clear main points with examples\n- Actionable takeaways\n- Compelling conclusion\n\n{{additional_notes?}}\n\nUse subheadings, bullet points where appropriate, and maintain reader engagement throughout.\"\"\",\n                tags=[\"writing\", \"blog\", \"content\"]\n            ),\n\n            PromptTemplate(\n                name=\"email_draft\",\n                category=TemplateCategory.WRITING,\n                description=\"Draft a professional email\",\n                template=\"\"\"Draft a professional email.\n\nPURPOSE: {{purpose}}\nRECIPIENT: {{recipient}}\nTONE: {{tone?professional}}\nKEY POINTS TO COVER:\n{{key_points}}\n\n{{context?}}\n\nThe email should be clear, concise, and achieve the intended purpose while maintaining appropriate professionalism.\"\"\",\n                tags=[\"writing\", \"email\", \"communication\"]\n            ),\n\n            PromptTemplate(\n                name=\"documentation\",\n                category=TemplateCategory.WRITING,\n                description=\"Write technical documentation\",\n                template=\"\"\"Write technical documentation for: {{subject}}\n\nDocumentation type: {{doc_type?user guide}}\nTarget audience: {{audience?developers}}\n\nInclude:\n- Overview/introduction\n- Key concepts\n- Step-by-step instructions where applicable\n- Examples\n- Common issues/troubleshooting\n\n{{specific_sections?}}\n\nUse clear formatting with headers, code blocks, and notes as appropriate.\"\"\",\n                tags=[\"writing\", \"documentation\", \"technical\"]\n            ),\n\n            PromptTemplate(\n                name=\"summary\",\n                category=TemplateCategory.WRITING,\n                description=\"Summarize content\",\n                template=\"\"\"Summarize the following content.\n\nCONTENT:\n{{content}}\n\nSummary requirements:\n- Length: {{length?2-3 paragraphs}}\n- Focus on: {{focus?key points and main arguments}}\n- Format: {{format?prose}}\n\n{{additional_requirements?}}\"\"\",\n                tags=[\"writing\", \"summary\", \"condensing\"]\n            ),\n\n            # ANALYSIS TEMPLATES\n            PromptTemplate(\n                name=\"data_analysis\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Analyze data or information\",\n                template=\"\"\"Analyze the following data/information:\n\nDATA:\n{{data}}\n\nAnalysis goals:\n{{goals}}\n\nPlease provide:\n1. Key findings and patterns\n2. Statistical insights (if applicable)\n3. Anomalies or notable observations\n4. Recommendations based on findings\n5. Limitations of this analysis\n\n{{output_format?Present findings in a structured report format}}\"\"\",\n                tags=[\"analysis\", \"data\", \"insights\"]\n            ),\n\n            PromptTemplate(\n                name=\"compare_contrast\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Compare and contrast items\",\n                template=\"\"\"Compare and contrast the following:\n\nITEM 1: {{item1}}\nITEM 2: {{item2}}\n{{item3?}}\n\nComparison criteria:\n{{criteria?all relevant aspects}}\n\nProvide:\n1. Side-by-side comparison table\n2. Key similarities\n3. Key differences\n4. Use case recommendations (when to choose each)\n5. Overall assessment\"\"\",\n                tags=[\"analysis\", \"comparison\", \"decision\"]\n            ),\n\n            PromptTemplate(\n                name=\"problem_solve\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Structured problem solving\",\n                template=\"\"\"Help me solve this problem:\n\nPROBLEM:\n{{problem}}\n\nCONTEXT:\n{{context}}\n\nCONSTRAINTS:\n{{constraints?None specified}}\n\nPlease provide:\n1. Problem breakdown and root cause analysis\n2. Multiple potential solutions (at least 3)\n3. Pros and cons of each solution\n4. Recommended approach with justification\n5. Implementation steps for the recommended solution\"\"\",\n                tags=[\"analysis\", \"problem-solving\", \"strategy\"]\n            ),\n\n            PromptTemplate(\n                name=\"research\",\n                category=TemplateCategory.ANALYSIS,\n                description=\"Research and synthesize information\",\n                template=\"\"\"Research and provide comprehensive information about:\n\nTOPIC: {{topic}}\n\nScope: {{scope?comprehensive overview}}\n\nPlease cover:\n1. Definition and background\n2. Key concepts and components\n3. Current state/trends\n4. Important considerations\n5. Resources for further learning\n\n{{specific_questions?}}\n\nCite sources where applicable and indicate confidence levels for claims.\"\"\",\n                tags=[\"analysis\", \"research\", \"learning\"]\n            ),\n\n            # CREATIVE TEMPLATES\n            PromptTemplate(\n                name=\"brainstorm\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Generate creative ideas\",\n                template=\"\"\"Brainstorm ideas for: {{topic}}\n\nContext: {{context?}}\nConstraints: {{constraints?none}}\nNumber of ideas: {{count?10}}\n\nFor each idea, provide:\n- Brief description\n- Key benefits\n- Potential challenges\n- Feasibility rating (1-5)\n\nThink outside the box and include both conventional and unconventional approaches.\"\"\",\n                tags=[\"creative\", \"ideation\", \"brainstorming\"]\n            ),\n\n            PromptTemplate(\n                name=\"story_outline\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Create a story outline\",\n                template=\"\"\"Create a story outline.\n\nGenre: {{genre}}\nSetting: {{setting}}\nMain theme: {{theme}}\nTarget audience: {{audience?general}}\n\nInclude:\n1. Premise (1-2 sentences)\n2. Main characters with brief descriptions\n3. Three-act structure outline\n4. Key plot points and conflicts\n5. Potential endings\n\n{{additional_elements?}}\"\"\",\n                tags=[\"creative\", \"writing\", \"storytelling\"]\n            ),\n\n            PromptTemplate(\n                name=\"name_generator\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Generate names for projects, products, etc.\",\n                template=\"\"\"Generate name ideas for: {{subject}}\n\nType: {{type?product/project}}\nStyle: {{style?modern and memorable}}\nRequirements: {{requirements?}}\n\nGenerate {{count?15}} name suggestions.\n\nFor each name, provide:\n- The name\n- Brief explanation of meaning/inspiration\n- Domain availability likelihood (if applicable)\n\nInclude a mix of:\n- Descriptive names\n- Abstract/creative names\n- Compound words\n- Invented words\"\"\",\n                tags=[\"creative\", \"naming\", \"branding\"]\n            ),\n\n            PromptTemplate(\n                name=\"rewrite\",\n                category=TemplateCategory.CREATIVE,\n                description=\"Rewrite content in a different style\",\n                template=\"\"\"Rewrite the following content.\n\nORIGINAL:\n{{content}}\n\nNEW STYLE: {{style}}\nTARGET AUDIENCE: {{audience?general}}\nTONE: {{tone?}}\n\nMaintain the core meaning while adapting to the new style.\n{{preserve?}}\"\"\",\n                tags=[\"creative\", \"writing\", \"adaptation\"]\n            ),\n        ]\n\n        for template in builtin:\n            self.templates[template.name] = template\n\n    def get(self, name: str) -> Optional[PromptTemplate]:\n        \"\"\"Get a template by name.\"\"\"\n        return self.templates.get(name)\n\n    def add(self, template: PromptTemplate) -> None:\n        \"\"\"Add a new template to the library.\"\"\"\n        self.templates[template.name] = template\n\n    def remove(self, name: str) -> bool:\n        \"\"\"Remove a template by name. Returns True if removed.\"\"\"\n        if name in self.templates:\n            del self.templates[name]\n            return True\n        return False\n\n    def list_templates(\n        self,\n        category: Optional[TemplateCategory] = None,\n        tags: Optional[list[str]] = None\n    ) -> list[PromptTemplate]:\n        \"\"\"\n        List templates, optionally filtered by category or tags.\n\n        Args:\n            category: Filter by category\n            tags: Filter by tags (any match)\n\n        Returns:\n            List of matching templates\n        \"\"\"\n        results = list(self.templates.values())\n\n        if category:\n            results = [t for t in results if t.category == category]\n\n        if tags:\n            results = [t for t in results if any(tag in t.tags for tag in tags)]\n\n        return sorted(results, key=lambda t: t.name)\n\n    def search(self, query: str) -> list[PromptTemplate]:\n        \"\"\"Search templates by name, description, or tags.\"\"\"\n        query = query.lower()\n        results = []\n\n        for template in self.templates.values():\n            if (query in template.name.lower() or\n                query in template.description.lower() or\n                any(query in tag.lower() for tag in template.tags)):\n                results.append(template)\n\n        return sorted(results, key=lambda t: t.name)\n\n    def render(self, name: str, **kwargs: Any) -> str:\n        \"\"\"\n        Render a template by name with provided variables.\n\n        Args:\n            name: Template name\n            **kwargs: Variable values\n\n        Returns:\n            Rendered prompt string\n\n        Raises:\n            KeyError: If template not found\n            ValueError: If required variables missing\n        \"\"\"\n        template = self.get(name)\n        if not template:\n            raise KeyError(f\"Template not found: {name}\")\n        return template.render(**kwargs)\n\n    def export_to_json(self, filepath: str) -> None:\n        \"\"\"Export all templates to a JSON file.\"\"\"\n        data = {name: t.to_dict() for name, t in self.templates.items()}\n        Path(filepath).write_text(json.dumps(data, indent=2))\n\n    def import_from_json(self, filepath: str) -> int:\n        \"\"\"\n        Import templates from a JSON file.\n\n        Returns:\n            Number of templates imported\n        \"\"\"\n        data = json.loads(Path(filepath).read_text())\n        count = 0\n        for name, template_data in data.items():\n            self.templates[name] = PromptTemplate.from_dict(template_data)\n            count += 1\n        return count\n\n    def get_categories(self) -> list[TemplateCategory]:\n        \"\"\"Get list of categories that have templates.\"\"\"\n        return list(set(t.category for t in self.templates.values()))\n\n    def get_all_tags(self) -> list[str]:\n        \"\"\"Get all unique tags across templates.\"\"\"\n        tags = set()\n        for template in self.templates.values():\n            tags.update(template.tags)\n        return sorted(tags)\n\n\n# Global library instance\nlibrary = TemplateLibrary()\n\n\ndef get_template(name: str) -> Optional[PromptTemplate]:\n    \"\"\"Get a template from the global library.\"\"\"\n    return library.get(name)\n\n\ndef render_template(name: str, **kwargs: Any) -> str:\n    \"\"\"Render a template from the global library.\"\"\"\n    return library.render(name, **kwargs)\n\n\ndef list_templates(category: Optional[str] = None) -> list[PromptTemplate]:\n    \"\"\"List templates, optionally filtered by category.\"\"\"\n    cat = TemplateCategory(category) if category else None\n    return library.list_templates(category=cat)\n\n\nif __name__ == \"__main__\":\n    # Demo usage\n    print(\"PromptWizz Template Library\")\n    print(\"=\" * 50)\n    print()\n\n    # List all categories\n    print(\"Categories:\")\n    for cat in library.get_categories():\n        templates = library.list_templates(category=cat)\n        print(f\"  {cat.value}: {len(templates)} templates\")\n\n    print()\n\n    # Show example template usage\n    print(\"Example - Code Review Template:\")\n    print(\"-\" * 50)\n\n    prompt = library.render(\n        \"code_review\",\n        language=\"python\",\n        code=\"\"\"def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\"\"\",\n        focus=\"Focus on performance optimization\"\n    )\n    print(prompt)\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"447649e7-0fbf-4605-9a3b-45583bb3e805"}
{"type":"file-history-snapshot","messageId":"fe5434b6-4e2e-4169-98dd-8028f8a99d65","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"},"ab_test.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:09.481Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"81abb4c9-72c3-4d7b-a9f1-a9dcead9e810","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MfMdgfHhcb344KbZSUBRu9","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/ab_test.py","content":"\"\"\"\nPromptWizz A/B Tester\n\nRun the same query with different prompts and compare outputs.\nTrack which prompts perform better over time.\n\"\"\"\n\nimport hashlib\nimport json\nimport statistics\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Callable, Optional\n\nimport anthropic\n\n\n@dataclass\nclass PromptVariant:\n    \"\"\"A prompt variant for testing.\"\"\"\n    name: str\n    prompt: str\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass TestResult:\n    \"\"\"Result from a single prompt test.\"\"\"\n    variant_name: str\n    prompt: str\n    response: str\n    latency_ms: float\n    token_count: int\n    timestamp: str\n    scores: dict[str, float] = field(default_factory=dict)\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass ComparisonResult:\n    \"\"\"Result from comparing multiple prompt variants.\"\"\"\n    query: str\n    variants: list[PromptVariant]\n    results: list[TestResult]\n    winner: Optional[str] = None\n    analysis: str = \"\"\n    scores_summary: dict[str, dict[str, float]] = field(default_factory=dict)\n\n    def summary(self) -> str:\n        \"\"\"Generate a human-readable summary.\"\"\"\n        lines = [\n            \"=\" * 60,\n            \"PROMPTWIZZ A/B TEST RESULTS\",\n            \"=\" * 60,\n            \"\",\n            f\"Query: {self.query[:100]}{'...' if len(self.query) > 100 else ''}\",\n            f\"Variants tested: {len(self.variants)}\",\n            \"\",\n            \"Results by Variant:\",\n            \"-\" * 40,\n        ]\n\n        for result in self.results:\n            lines.extend([\n                f\"\\n{result.variant_name}:\",\n                f\"  Latency: {result.latency_ms:.0f}ms\",\n                f\"  Tokens: {result.token_count}\",\n            ])\n            if result.scores:\n                for metric, score in result.scores.items():\n                    lines.append(f\"  {metric}: {score:.2f}\")\n\n        if self.winner:\n            lines.extend([\n                \"\",\n                \"-\" * 40,\n                f\"WINNER: {self.winner}\",\n            ])\n\n        if self.analysis:\n            lines.extend([\n                \"\",\n                \"Analysis:\",\n                self.analysis\n            ])\n\n        lines.append(\"=\" * 60)\n        return \"\\n\".join(lines)\n\n\n@dataclass\nclass TestHistory:\n    \"\"\"Historical record of A/B test results.\"\"\"\n    test_id: str\n    query: str\n    variants: list[str]\n    winner: Optional[str]\n    timestamp: str\n    scores: dict[str, dict[str, float]]\n\n\nclass ABTester:\n    \"\"\"\n    A/B testing system for comparing prompt effectiveness.\n\n    Features:\n    - Run same query with multiple prompt variants\n    - Score outputs using built-in or custom metrics\n    - AI-powered quality comparison\n    - Track historical performance\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        history_file: Optional[str] = None,\n        model: str = \"claude-sonnet-4-20250514\"\n    ):\n        \"\"\"\n        Initialize the A/B tester.\n\n        Args:\n            api_key: Anthropic API key\n            history_file: Path to store test history (JSON)\n            model: Claude model to use for tests\n        \"\"\"\n        self.client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n        self.model = model\n        self.history_file = history_file\n        self.history: list[TestHistory] = []\n\n        if history_file and Path(history_file).exists():\n            self._load_history()\n\n    def _load_history(self) -> None:\n        \"\"\"Load test history from file.\"\"\"\n        if self.history_file:\n            try:\n                data = json.loads(Path(self.history_file).read_text())\n                self.history = [\n                    TestHistory(**item) for item in data\n                ]\n            except Exception:\n                self.history = []\n\n    def _save_history(self) -> None:\n        \"\"\"Save test history to file.\"\"\"\n        if self.history_file:\n            data = [\n                {\n                    \"test_id\": h.test_id,\n                    \"query\": h.query,\n                    \"variants\": h.variants,\n                    \"winner\": h.winner,\n                    \"timestamp\": h.timestamp,\n                    \"scores\": h.scores\n                }\n                for h in self.history\n            ]\n            Path(self.history_file).write_text(json.dumps(data, indent=2))\n\n    def _generate_test_id(self, query: str, variants: list[str]) -> str:\n        \"\"\"Generate unique test ID.\"\"\"\n        content = query + \"\".join(sorted(variants))\n        return hashlib.sha256(content.encode()).hexdigest()[:12]\n\n    def run_test(\n        self,\n        query: str,\n        variants: list[PromptVariant],\n        metrics: Optional[list[Callable[[str, str], float]]] = None,\n        use_ai_judge: bool = True,\n        max_tokens: int = 1024\n    ) -> ComparisonResult:\n        \"\"\"\n        Run A/B test with multiple prompt variants.\n\n        Args:\n            query: The user query/task to test\n            variants: List of prompt variants to compare\n            metrics: Optional custom scoring functions (prompt, response) -> score\n            use_ai_judge: Use Claude to judge quality\n            max_tokens: Max tokens for responses\n\n        Returns:\n            ComparisonResult with all test data\n        \"\"\"\n        results = []\n\n        # Run each variant\n        for variant in variants:\n            # Construct full prompt\n            full_prompt = variant.prompt.format(query=query) if \"{query}\" in variant.prompt else f\"{variant.prompt}\\n\\n{query}\"\n\n            # Time the API call\n            start_time = time.time()\n\n            try:\n                response = self.client.messages.create(\n                    model=self.model,\n                    max_tokens=max_tokens,\n                    messages=[{\"role\": \"user\", \"content\": full_prompt}]\n                )\n\n                latency_ms = (time.time() - start_time) * 1000\n                response_text = response.content[0].text\n                token_count = response.usage.output_tokens\n\n                # Calculate metrics\n                scores = {}\n\n                # Built-in metrics\n                scores[\"response_length\"] = len(response_text)\n                scores[\"word_count\"] = len(response_text.split())\n\n                # Custom metrics\n                if metrics:\n                    for i, metric_fn in enumerate(metrics):\n                        try:\n                            scores[f\"custom_metric_{i+1}\"] = metric_fn(full_prompt, response_text)\n                        except Exception:\n                            pass\n\n                result = TestResult(\n                    variant_name=variant.name,\n                    prompt=full_prompt,\n                    response=response_text,\n                    latency_ms=latency_ms,\n                    token_count=token_count,\n                    timestamp=datetime.now().isoformat(),\n                    scores=scores,\n                    metadata=variant.metadata\n                )\n                results.append(result)\n\n            except Exception as e:\n                print(f\"Error testing variant '{variant.name}': {e}\")\n                continue\n\n        # Create comparison result\n        comparison = ComparisonResult(\n            query=query,\n            variants=variants,\n            results=results\n        )\n\n        # Use AI to judge if enabled\n        if use_ai_judge and len(results) >= 2:\n            ai_judgment = self._ai_judge(query, results)\n            comparison.winner = ai_judgment.get(\"winner\")\n            comparison.analysis = ai_judgment.get(\"analysis\", \"\")\n\n            # Add AI scores to results\n            for result in results:\n                if result.variant_name in ai_judgment.get(\"scores\", {}):\n                    result.scores[\"ai_quality\"] = ai_judgment[\"scores\"][result.variant_name]\n\n        # Calculate summary scores\n        for result in results:\n            comparison.scores_summary[result.variant_name] = result.scores.copy()\n\n        # Save to history\n        test_id = self._generate_test_id(query, [v.name for v in variants])\n        self.history.append(TestHistory(\n            test_id=test_id,\n            query=query,\n            variants=[v.name for v in variants],\n            winner=comparison.winner,\n            timestamp=datetime.now().isoformat(),\n            scores=comparison.scores_summary\n        ))\n        self._save_history()\n\n        return comparison\n\n    def _ai_judge(self, query: str, results: list[TestResult]) -> dict:\n        \"\"\"Use Claude to judge response quality.\"\"\"\n        responses_text = \"\"\n        for i, result in enumerate(results, 1):\n            responses_text += f\"\"\"\nRESPONSE {i} (Variant: {result.variant_name}):\n{result.response[:2000]}{'...' if len(result.response) > 2000 else ''}\n\n\"\"\"\n\n        judge_prompt = f\"\"\"You are an expert at evaluating AI responses. Compare these responses to the same query.\n\nQUERY: {query}\n\n{responses_text}\n\nEvaluate each response on:\n1. Accuracy and correctness\n2. Completeness\n3. Clarity and organization\n4. Usefulness to the user\n\nRespond in this exact format:\nWINNER: [variant name]\nSCORES:\n- [variant1_name]: [score 1-10]\n- [variant2_name]: [score 1-10]\nANALYSIS:\n[2-3 sentences explaining why the winner is better]\"\"\"\n\n        try:\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=500,\n                messages=[{\"role\": \"user\", \"content\": judge_prompt}]\n            )\n\n            content = response.content[0].text\n            result = {\"scores\": {}}\n\n            # Parse winner\n            if \"WINNER:\" in content:\n                winner_line = content.split(\"WINNER:\")[1].split(\"\\n\")[0].strip()\n                result[\"winner\"] = winner_line\n\n            # Parse scores\n            if \"SCORES:\" in content:\n                scores_section = content.split(\"SCORES:\")[1]\n                if \"ANALYSIS:\" in scores_section:\n                    scores_section = scores_section.split(\"ANALYSIS:\")[0]\n\n                for line in scores_section.split(\"\\n\"):\n                    line = line.strip()\n                    if line.startswith(\"-\") and \":\" in line:\n                        parts = line[1:].split(\":\")\n                        if len(parts) >= 2:\n                            name = parts[0].strip()\n                            try:\n                                score = float(parts[1].strip().split()[0])\n                                result[\"scores\"][name] = score\n                            except ValueError:\n                                pass\n\n            # Parse analysis\n            if \"ANALYSIS:\" in content:\n                result[\"analysis\"] = content.split(\"ANALYSIS:\")[1].strip()\n\n            return result\n\n        except Exception as e:\n            print(f\"AI judge error: {e}\")\n            return {}\n\n    def quick_compare(\n        self,\n        query: str,\n        prompt_a: str,\n        prompt_b: str\n    ) -> ComparisonResult:\n        \"\"\"\n        Quick comparison of two prompts.\n\n        Args:\n            query: The query to test\n            prompt_a: First prompt variant\n            prompt_b: Second prompt variant\n\n        Returns:\n            ComparisonResult\n        \"\"\"\n        variants = [\n            PromptVariant(name=\"Variant A\", prompt=prompt_a),\n            PromptVariant(name=\"Variant B\", prompt=prompt_b)\n        ]\n        return self.run_test(query, variants)\n\n    def get_performance_stats(self, variant_name: str) -> dict:\n        \"\"\"\n        Get historical performance statistics for a variant.\n\n        Args:\n            variant_name: Name of the variant to analyze\n\n        Returns:\n            Dict with win rate, average scores, etc.\n        \"\"\"\n        relevant_tests = [\n            h for h in self.history\n            if variant_name in h.variants\n        ]\n\n        if not relevant_tests:\n            return {\"error\": \"No tests found for this variant\"}\n\n        wins = sum(1 for h in relevant_tests if h.winner == variant_name)\n\n        all_scores = []\n        for h in relevant_tests:\n            if variant_name in h.scores:\n                variant_scores = h.scores[variant_name]\n                if \"ai_quality\" in variant_scores:\n                    all_scores.append(variant_scores[\"ai_quality\"])\n\n        return {\n            \"total_tests\": len(relevant_tests),\n            \"wins\": wins,\n            \"win_rate\": wins / len(relevant_tests) if relevant_tests else 0,\n            \"avg_quality_score\": statistics.mean(all_scores) if all_scores else None,\n            \"score_std_dev\": statistics.stdev(all_scores) if len(all_scores) > 1 else None\n        }\n\n    def get_best_prompts(self, limit: int = 5) -> list[dict]:\n        \"\"\"\n        Get the best-performing prompt variants from history.\n\n        Args:\n            limit: Maximum number to return\n\n        Returns:\n            List of variant performance data\n        \"\"\"\n        variant_stats = {}\n\n        for h in self.history:\n            for variant in h.variants:\n                if variant not in variant_stats:\n                    variant_stats[variant] = {\"wins\": 0, \"total\": 0}\n                variant_stats[variant][\"total\"] += 1\n                if h.winner == variant:\n                    variant_stats[variant][\"wins\"] += 1\n\n        # Calculate win rates\n        results = []\n        for variant, stats in variant_stats.items():\n            results.append({\n                \"variant\": variant,\n                \"win_rate\": stats[\"wins\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0,\n                \"wins\": stats[\"wins\"],\n                \"total_tests\": stats[\"total\"]\n            })\n\n        # Sort by win rate\n        results.sort(key=lambda x: x[\"win_rate\"], reverse=True)\n        return results[:limit]\n\n\ndef create_variants(\n    base_prompt: str,\n    modifications: dict[str, str]\n) -> list[PromptVariant]:\n    \"\"\"\n    Helper to create prompt variants from a base and modifications.\n\n    Args:\n        base_prompt: The base prompt template\n        modifications: Dict of {variant_name: modified_prompt}\n\n    Returns:\n        List of PromptVariant objects\n    \"\"\"\n    variants = [PromptVariant(name=\"baseline\", prompt=base_prompt)]\n    for name, prompt in modifications.items():\n        variants.append(PromptVariant(name=name, prompt=prompt))\n    return variants\n\n\n# Preset prompt strategies for common comparisons\nPROMPT_STRATEGIES = {\n    \"direct\": \"{query}\",\n\n    \"detailed\": \"\"\"Please provide a comprehensive and detailed response to the following:\n\n{query}\n\nInclude relevant examples, explanations, and considerations.\"\"\",\n\n    \"step_by_step\": \"\"\"Think through this step by step:\n\n{query}\n\nBreak down your reasoning and provide a clear, logical response.\"\"\",\n\n    \"expert\": \"\"\"You are a world-class expert in this domain.\n\n{query}\n\nProvide your expert analysis and recommendations.\"\"\",\n\n    \"structured\": \"\"\"Respond to the following query in a well-structured format:\n\n{query}\n\nUse headers, bullet points, and clear organization in your response.\"\"\",\n\n    \"concise\": \"\"\"Be concise and direct. Answer this briefly:\n\n{query}\n\nFocus on the most important points only.\"\"\",\n}\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    print(\"PromptWizz A/B Tester\")\n    print(\"=\" * 50)\n\n    # Create test variants\n    variants = [\n        PromptVariant(\n            name=\"direct\",\n            prompt=PROMPT_STRATEGIES[\"direct\"]\n        ),\n        PromptVariant(\n            name=\"step_by_step\",\n            prompt=PROMPT_STRATEGIES[\"step_by_step\"]\n        ),\n        PromptVariant(\n            name=\"expert\",\n            prompt=PROMPT_STRATEGIES[\"expert\"]\n        )\n    ]\n\n    print(\"\\nAvailable prompt strategies:\")\n    for name, prompt in PROMPT_STRATEGIES.items():\n        print(f\"  - {name}\")\n\n    print(\"\\nExample usage:\")\n    print(\"\"\"\n    tester = ABTester()\n    result = tester.run_test(\n        query=\"Explain how neural networks learn\",\n        variants=variants\n    )\n    print(result.summary())\n    \"\"\")\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"fe5434b6-4e2e-4169-98dd-8028f8a99d65","timestamp":"2026-02-01T03:19:09.463Z"}
{"parentUuid":"fe5434b6-4e2e-4169-98dd-8028f8a99d65","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01MfMdgfHhcb344KbZSUBRu9","toolUseID":"toolu_01MfMdgfHhcb344KbZSUBRu9","timestamp":"2026-02-01T03:19:09.516Z","uuid":"011b37b9-a4f9-44c9-914e-4549bc9907ce"}
{"parentUuid":"fe5434b6-4e2e-4169-98dd-8028f8a99d65","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MfMdgfHhcb344KbZSUBRu9","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/ab_test.py"}]},"uuid":"cf2920dc-c990-4b55-b491-63138bf12af7","timestamp":"2026-02-01T03:19:09.512Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/ab_test.py","content":"\"\"\"\nPromptWizz A/B Tester\n\nRun the same query with different prompts and compare outputs.\nTrack which prompts perform better over time.\n\"\"\"\n\nimport hashlib\nimport json\nimport statistics\nimport time\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Callable, Optional\n\nimport anthropic\n\n\n@dataclass\nclass PromptVariant:\n    \"\"\"A prompt variant for testing.\"\"\"\n    name: str\n    prompt: str\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass TestResult:\n    \"\"\"Result from a single prompt test.\"\"\"\n    variant_name: str\n    prompt: str\n    response: str\n    latency_ms: float\n    token_count: int\n    timestamp: str\n    scores: dict[str, float] = field(default_factory=dict)\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass ComparisonResult:\n    \"\"\"Result from comparing multiple prompt variants.\"\"\"\n    query: str\n    variants: list[PromptVariant]\n    results: list[TestResult]\n    winner: Optional[str] = None\n    analysis: str = \"\"\n    scores_summary: dict[str, dict[str, float]] = field(default_factory=dict)\n\n    def summary(self) -> str:\n        \"\"\"Generate a human-readable summary.\"\"\"\n        lines = [\n            \"=\" * 60,\n            \"PROMPTWIZZ A/B TEST RESULTS\",\n            \"=\" * 60,\n            \"\",\n            f\"Query: {self.query[:100]}{'...' if len(self.query) > 100 else ''}\",\n            f\"Variants tested: {len(self.variants)}\",\n            \"\",\n            \"Results by Variant:\",\n            \"-\" * 40,\n        ]\n\n        for result in self.results:\n            lines.extend([\n                f\"\\n{result.variant_name}:\",\n                f\"  Latency: {result.latency_ms:.0f}ms\",\n                f\"  Tokens: {result.token_count}\",\n            ])\n            if result.scores:\n                for metric, score in result.scores.items():\n                    lines.append(f\"  {metric}: {score:.2f}\")\n\n        if self.winner:\n            lines.extend([\n                \"\",\n                \"-\" * 40,\n                f\"WINNER: {self.winner}\",\n            ])\n\n        if self.analysis:\n            lines.extend([\n                \"\",\n                \"Analysis:\",\n                self.analysis\n            ])\n\n        lines.append(\"=\" * 60)\n        return \"\\n\".join(lines)\n\n\n@dataclass\nclass TestHistory:\n    \"\"\"Historical record of A/B test results.\"\"\"\n    test_id: str\n    query: str\n    variants: list[str]\n    winner: Optional[str]\n    timestamp: str\n    scores: dict[str, dict[str, float]]\n\n\nclass ABTester:\n    \"\"\"\n    A/B testing system for comparing prompt effectiveness.\n\n    Features:\n    - Run same query with multiple prompt variants\n    - Score outputs using built-in or custom metrics\n    - AI-powered quality comparison\n    - Track historical performance\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        history_file: Optional[str] = None,\n        model: str = \"claude-sonnet-4-20250514\"\n    ):\n        \"\"\"\n        Initialize the A/B tester.\n\n        Args:\n            api_key: Anthropic API key\n            history_file: Path to store test history (JSON)\n            model: Claude model to use for tests\n        \"\"\"\n        self.client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n        self.model = model\n        self.history_file = history_file\n        self.history: list[TestHistory] = []\n\n        if history_file and Path(history_file).exists():\n            self._load_history()\n\n    def _load_history(self) -> None:\n        \"\"\"Load test history from file.\"\"\"\n        if self.history_file:\n            try:\n                data = json.loads(Path(self.history_file).read_text())\n                self.history = [\n                    TestHistory(**item) for item in data\n                ]\n            except Exception:\n                self.history = []\n\n    def _save_history(self) -> None:\n        \"\"\"Save test history to file.\"\"\"\n        if self.history_file:\n            data = [\n                {\n                    \"test_id\": h.test_id,\n                    \"query\": h.query,\n                    \"variants\": h.variants,\n                    \"winner\": h.winner,\n                    \"timestamp\": h.timestamp,\n                    \"scores\": h.scores\n                }\n                for h in self.history\n            ]\n            Path(self.history_file).write_text(json.dumps(data, indent=2))\n\n    def _generate_test_id(self, query: str, variants: list[str]) -> str:\n        \"\"\"Generate unique test ID.\"\"\"\n        content = query + \"\".join(sorted(variants))\n        return hashlib.sha256(content.encode()).hexdigest()[:12]\n\n    def run_test(\n        self,\n        query: str,\n        variants: list[PromptVariant],\n        metrics: Optional[list[Callable[[str, str], float]]] = None,\n        use_ai_judge: bool = True,\n        max_tokens: int = 1024\n    ) -> ComparisonResult:\n        \"\"\"\n        Run A/B test with multiple prompt variants.\n\n        Args:\n            query: The user query/task to test\n            variants: List of prompt variants to compare\n            metrics: Optional custom scoring functions (prompt, response) -> score\n            use_ai_judge: Use Claude to judge quality\n            max_tokens: Max tokens for responses\n\n        Returns:\n            ComparisonResult with all test data\n        \"\"\"\n        results = []\n\n        # Run each variant\n        for variant in variants:\n            # Construct full prompt\n            full_prompt = variant.prompt.format(query=query) if \"{query}\" in variant.prompt else f\"{variant.prompt}\\n\\n{query}\"\n\n            # Time the API call\n            start_time = time.time()\n\n            try:\n                response = self.client.messages.create(\n                    model=self.model,\n                    max_tokens=max_tokens,\n                    messages=[{\"role\": \"user\", \"content\": full_prompt}]\n                )\n\n                latency_ms = (time.time() - start_time) * 1000\n                response_text = response.content[0].text\n                token_count = response.usage.output_tokens\n\n                # Calculate metrics\n                scores = {}\n\n                # Built-in metrics\n                scores[\"response_length\"] = len(response_text)\n                scores[\"word_count\"] = len(response_text.split())\n\n                # Custom metrics\n                if metrics:\n                    for i, metric_fn in enumerate(metrics):\n                        try:\n                            scores[f\"custom_metric_{i+1}\"] = metric_fn(full_prompt, response_text)\n                        except Exception:\n                            pass\n\n                result = TestResult(\n                    variant_name=variant.name,\n                    prompt=full_prompt,\n                    response=response_text,\n                    latency_ms=latency_ms,\n                    token_count=token_count,\n                    timestamp=datetime.now().isoformat(),\n                    scores=scores,\n                    metadata=variant.metadata\n                )\n                results.append(result)\n\n            except Exception as e:\n                print(f\"Error testing variant '{variant.name}': {e}\")\n                continue\n\n        # Create comparison result\n        comparison = ComparisonResult(\n            query=query,\n            variants=variants,\n            results=results\n        )\n\n        # Use AI to judge if enabled\n        if use_ai_judge and len(results) >= 2:\n            ai_judgment = self._ai_judge(query, results)\n            comparison.winner = ai_judgment.get(\"winner\")\n            comparison.analysis = ai_judgment.get(\"analysis\", \"\")\n\n            # Add AI scores to results\n            for result in results:\n                if result.variant_name in ai_judgment.get(\"scores\", {}):\n                    result.scores[\"ai_quality\"] = ai_judgment[\"scores\"][result.variant_name]\n\n        # Calculate summary scores\n        for result in results:\n            comparison.scores_summary[result.variant_name] = result.scores.copy()\n\n        # Save to history\n        test_id = self._generate_test_id(query, [v.name for v in variants])\n        self.history.append(TestHistory(\n            test_id=test_id,\n            query=query,\n            variants=[v.name for v in variants],\n            winner=comparison.winner,\n            timestamp=datetime.now().isoformat(),\n            scores=comparison.scores_summary\n        ))\n        self._save_history()\n\n        return comparison\n\n    def _ai_judge(self, query: str, results: list[TestResult]) -> dict:\n        \"\"\"Use Claude to judge response quality.\"\"\"\n        responses_text = \"\"\n        for i, result in enumerate(results, 1):\n            responses_text += f\"\"\"\nRESPONSE {i} (Variant: {result.variant_name}):\n{result.response[:2000]}{'...' if len(result.response) > 2000 else ''}\n\n\"\"\"\n\n        judge_prompt = f\"\"\"You are an expert at evaluating AI responses. Compare these responses to the same query.\n\nQUERY: {query}\n\n{responses_text}\n\nEvaluate each response on:\n1. Accuracy and correctness\n2. Completeness\n3. Clarity and organization\n4. Usefulness to the user\n\nRespond in this exact format:\nWINNER: [variant name]\nSCORES:\n- [variant1_name]: [score 1-10]\n- [variant2_name]: [score 1-10]\nANALYSIS:\n[2-3 sentences explaining why the winner is better]\"\"\"\n\n        try:\n            response = self.client.messages.create(\n                model=self.model,\n                max_tokens=500,\n                messages=[{\"role\": \"user\", \"content\": judge_prompt}]\n            )\n\n            content = response.content[0].text\n            result = {\"scores\": {}}\n\n            # Parse winner\n            if \"WINNER:\" in content:\n                winner_line = content.split(\"WINNER:\")[1].split(\"\\n\")[0].strip()\n                result[\"winner\"] = winner_line\n\n            # Parse scores\n            if \"SCORES:\" in content:\n                scores_section = content.split(\"SCORES:\")[1]\n                if \"ANALYSIS:\" in scores_section:\n                    scores_section = scores_section.split(\"ANALYSIS:\")[0]\n\n                for line in scores_section.split(\"\\n\"):\n                    line = line.strip()\n                    if line.startswith(\"-\") and \":\" in line:\n                        parts = line[1:].split(\":\")\n                        if len(parts) >= 2:\n                            name = parts[0].strip()\n                            try:\n                                score = float(parts[1].strip().split()[0])\n                                result[\"scores\"][name] = score\n                            except ValueError:\n                                pass\n\n            # Parse analysis\n            if \"ANALYSIS:\" in content:\n                result[\"analysis\"] = content.split(\"ANALYSIS:\")[1].strip()\n\n            return result\n\n        except Exception as e:\n            print(f\"AI judge error: {e}\")\n            return {}\n\n    def quick_compare(\n        self,\n        query: str,\n        prompt_a: str,\n        prompt_b: str\n    ) -> ComparisonResult:\n        \"\"\"\n        Quick comparison of two prompts.\n\n        Args:\n            query: The query to test\n            prompt_a: First prompt variant\n            prompt_b: Second prompt variant\n\n        Returns:\n            ComparisonResult\n        \"\"\"\n        variants = [\n            PromptVariant(name=\"Variant A\", prompt=prompt_a),\n            PromptVariant(name=\"Variant B\", prompt=prompt_b)\n        ]\n        return self.run_test(query, variants)\n\n    def get_performance_stats(self, variant_name: str) -> dict:\n        \"\"\"\n        Get historical performance statistics for a variant.\n\n        Args:\n            variant_name: Name of the variant to analyze\n\n        Returns:\n            Dict with win rate, average scores, etc.\n        \"\"\"\n        relevant_tests = [\n            h for h in self.history\n            if variant_name in h.variants\n        ]\n\n        if not relevant_tests:\n            return {\"error\": \"No tests found for this variant\"}\n\n        wins = sum(1 for h in relevant_tests if h.winner == variant_name)\n\n        all_scores = []\n        for h in relevant_tests:\n            if variant_name in h.scores:\n                variant_scores = h.scores[variant_name]\n                if \"ai_quality\" in variant_scores:\n                    all_scores.append(variant_scores[\"ai_quality\"])\n\n        return {\n            \"total_tests\": len(relevant_tests),\n            \"wins\": wins,\n            \"win_rate\": wins / len(relevant_tests) if relevant_tests else 0,\n            \"avg_quality_score\": statistics.mean(all_scores) if all_scores else None,\n            \"score_std_dev\": statistics.stdev(all_scores) if len(all_scores) > 1 else None\n        }\n\n    def get_best_prompts(self, limit: int = 5) -> list[dict]:\n        \"\"\"\n        Get the best-performing prompt variants from history.\n\n        Args:\n            limit: Maximum number to return\n\n        Returns:\n            List of variant performance data\n        \"\"\"\n        variant_stats = {}\n\n        for h in self.history:\n            for variant in h.variants:\n                if variant not in variant_stats:\n                    variant_stats[variant] = {\"wins\": 0, \"total\": 0}\n                variant_stats[variant][\"total\"] += 1\n                if h.winner == variant:\n                    variant_stats[variant][\"wins\"] += 1\n\n        # Calculate win rates\n        results = []\n        for variant, stats in variant_stats.items():\n            results.append({\n                \"variant\": variant,\n                \"win_rate\": stats[\"wins\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0,\n                \"wins\": stats[\"wins\"],\n                \"total_tests\": stats[\"total\"]\n            })\n\n        # Sort by win rate\n        results.sort(key=lambda x: x[\"win_rate\"], reverse=True)\n        return results[:limit]\n\n\ndef create_variants(\n    base_prompt: str,\n    modifications: dict[str, str]\n) -> list[PromptVariant]:\n    \"\"\"\n    Helper to create prompt variants from a base and modifications.\n\n    Args:\n        base_prompt: The base prompt template\n        modifications: Dict of {variant_name: modified_prompt}\n\n    Returns:\n        List of PromptVariant objects\n    \"\"\"\n    variants = [PromptVariant(name=\"baseline\", prompt=base_prompt)]\n    for name, prompt in modifications.items():\n        variants.append(PromptVariant(name=name, prompt=prompt))\n    return variants\n\n\n# Preset prompt strategies for common comparisons\nPROMPT_STRATEGIES = {\n    \"direct\": \"{query}\",\n\n    \"detailed\": \"\"\"Please provide a comprehensive and detailed response to the following:\n\n{query}\n\nInclude relevant examples, explanations, and considerations.\"\"\",\n\n    \"step_by_step\": \"\"\"Think through this step by step:\n\n{query}\n\nBreak down your reasoning and provide a clear, logical response.\"\"\",\n\n    \"expert\": \"\"\"You are a world-class expert in this domain.\n\n{query}\n\nProvide your expert analysis and recommendations.\"\"\",\n\n    \"structured\": \"\"\"Respond to the following query in a well-structured format:\n\n{query}\n\nUse headers, bullet points, and clear organization in your response.\"\"\",\n\n    \"concise\": \"\"\"Be concise and direct. Answer this briefly:\n\n{query}\n\nFocus on the most important points only.\"\"\",\n}\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    print(\"PromptWizz A/B Tester\")\n    print(\"=\" * 50)\n\n    # Create test variants\n    variants = [\n        PromptVariant(\n            name=\"direct\",\n            prompt=PROMPT_STRATEGIES[\"direct\"]\n        ),\n        PromptVariant(\n            name=\"step_by_step\",\n            prompt=PROMPT_STRATEGIES[\"step_by_step\"]\n        ),\n        PromptVariant(\n            name=\"expert\",\n            prompt=PROMPT_STRATEGIES[\"expert\"]\n        )\n    ]\n\n    print(\"\\nAvailable prompt strategies:\")\n    for name, prompt in PROMPT_STRATEGIES.items():\n        print(f\"  - {name}\")\n\n    print(\"\\nExample usage:\")\n    print(\"\"\"\n    tester = ABTester()\n    result = tester.run_test(\n        query=\"Explain how neural networks learn\",\n        variants=variants\n    )\n    print(result.summary())\n    \"\"\")\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"fe5434b6-4e2e-4169-98dd-8028f8a99d65"}
{"type":"file-history-snapshot","messageId":"6d0d8f9e-0017-4636-8b99-302f58a02da7","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"},"ab_test.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:09.481Z"},"cli.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:53.717Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"cf2920dc-c990-4b55-b491-63138bf12af7","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FfZ7qcY2LgFuKR8YfKirA7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0162z8PKmgJZ54XGyWDRqXga","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/cli.py","content":"#!/usr/bin/env python3\n\"\"\"\nPromptWizz CLI\n\nCommand-line interface for the PromptWizz prompt engineering toolkit.\nProvides access to prompt analysis, templates, and A/B testing.\n\"\"\"\n\nimport argparse\nimport json\nimport sys\nfrom pathlib import Path\n\nfrom analyzer import PromptAnalyzer, analyze_prompt\nfrom templates import TemplateLibrary, TemplateCategory, library\nfrom ab_test import ABTester, PromptVariant, PROMPT_STRATEGIES\n\n\ndef cmd_analyze(args):\n    \"\"\"Analyze a prompt for quality.\"\"\"\n    # Get prompt from args or stdin\n    if args.prompt:\n        prompt = args.prompt\n    elif args.file:\n        prompt = Path(args.file).read_text()\n    else:\n        print(\"Enter your prompt (Ctrl+D to finish):\")\n        prompt = sys.stdin.read()\n\n    if not prompt.strip():\n        print(\"Error: No prompt provided\")\n        return 1\n\n    print(\"\\nAnalyzing prompt...\")\n    analyzer = PromptAnalyzer(use_ai=args.ai)\n    result = analyzer.analyze(prompt)\n\n    if args.json:\n        output = {\n            \"original_prompt\": result.original_prompt,\n            \"overall_score\": result.overall_score,\n            \"scores\": result.scores,\n            \"weaknesses\": [\n                {\"category\": w.category, \"description\": w.description, \"severity\": w.severity}\n                for w in result.weaknesses\n            ],\n            \"improvements\": [\n                {\"category\": i.category, \"suggestion\": i.suggestion, \"priority\": i.priority}\n                for i in result.improvements\n            ],\n            \"improved_prompt\": result.improved_prompt\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        print(result.summary())\n\n        if result.improved_prompt:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"IMPROVED PROMPT:\")\n            print(\"=\" * 60)\n            print(result.improved_prompt)\n\n    return 0\n\n\ndef cmd_score(args):\n    \"\"\"Quick score a prompt.\"\"\"\n    if args.prompt:\n        prompt = args.prompt\n    else:\n        print(\"Enter your prompt (Ctrl+D to finish):\")\n        prompt = sys.stdin.read()\n\n    if not prompt.strip():\n        print(\"Error: No prompt provided\")\n        return 1\n\n    analyzer = PromptAnalyzer(use_ai=False)\n    score = analyzer.quick_score(prompt)\n\n    if args.json:\n        print(json.dumps({\"score\": score}))\n    else:\n        bar = \"█\" * int(score / 10) + \"░\" * (10 - int(score / 10))\n        print(f\"\\nPrompt Score: [{bar}] {score:.1f}/100\")\n\n        if score < 40:\n            print(\"Quality: Poor - Needs significant improvement\")\n        elif score < 60:\n            print(\"Quality: Fair - Has room for improvement\")\n        elif score < 80:\n            print(\"Quality: Good - Solid prompt with minor issues\")\n        else:\n            print(\"Quality: Excellent - Well-crafted prompt\")\n\n    return 0\n\n\ndef cmd_templates(args):\n    \"\"\"List or show templates.\"\"\"\n    if args.list:\n        # List all templates\n        category = TemplateCategory(args.category) if args.category else None\n        templates = library.list_templates(category=category)\n\n        if args.json:\n            output = [\n                {\n                    \"name\": t.name,\n                    \"category\": t.category.value,\n                    \"description\": t.description,\n                    \"variables\": t.variables,\n                    \"tags\": t.tags\n                }\n                for t in templates\n            ]\n            print(json.dumps(output, indent=2))\n        else:\n            print(\"\\nAvailable Templates:\")\n            print(\"=\" * 60)\n\n            current_cat = None\n            for t in sorted(templates, key=lambda x: (x.category.value, x.name)):\n                if t.category != current_cat:\n                    current_cat = t.category\n                    print(f\"\\n[{current_cat.value.upper()}]\")\n\n                vars_str = \", \".join(t.variables[:3])\n                if len(t.variables) > 3:\n                    vars_str += \"...\"\n\n                print(f\"  {t.name}\")\n                print(f\"    {t.description}\")\n                print(f\"    Variables: {vars_str}\")\n\n        return 0\n\n    elif args.show:\n        # Show specific template\n        template = library.get(args.show)\n\n        if not template:\n            print(f\"Error: Template '{args.show}' not found\")\n            return 1\n\n        if args.json:\n            print(json.dumps(template.to_dict(), indent=2))\n        else:\n            print(f\"\\nTemplate: {template.name}\")\n            print(\"=\" * 60)\n            print(f\"Category: {template.category.value}\")\n            print(f\"Description: {template.description}\")\n            print(f\"\\nRequired Variables: {', '.join(template.variables)}\")\n            if template.optional_variables:\n                opt_vars = [f\"{k} (default: {v})\" for k, v in template.optional_variables.items()]\n                print(f\"Optional Variables: {', '.join(opt_vars)}\")\n            print(f\"Tags: {', '.join(template.tags)}\")\n            print(f\"\\nTemplate:\")\n            print(\"-\" * 40)\n            print(template.template)\n\n        return 0\n\n    elif args.render:\n        # Render a template with variables\n        template = library.get(args.render)\n\n        if not template:\n            print(f\"Error: Template '{args.render}' not found\")\n            return 1\n\n        # Parse variables from --var key=value arguments\n        variables = {}\n        if args.var:\n            for var_str in args.var:\n                if \"=\" in var_str:\n                    key, value = var_str.split(\"=\", 1)\n                    variables[key.strip()] = value.strip()\n\n        try:\n            rendered = template.render(**variables)\n\n            if args.json:\n                print(json.dumps({\"prompt\": rendered}))\n            else:\n                print(\"\\nRendered Prompt:\")\n                print(\"=\" * 60)\n                print(rendered)\n\n            return 0\n\n        except ValueError as e:\n            print(f\"Error: {e}\")\n            print(f\"\\nRequired variables: {', '.join(template.variables)}\")\n            return 1\n\n    else:\n        # Default: list categories\n        print(\"\\nTemplate Categories:\")\n        for cat in library.get_categories():\n            count = len(library.list_templates(category=cat))\n            print(f\"  {cat.value}: {count} templates\")\n        print(\"\\nUse --list to see all templates or --show <name> to view a specific template\")\n        return 0\n\n\ndef cmd_abtest(args):\n    \"\"\"Run A/B tests on prompts.\"\"\"\n    tester = ABTester(history_file=args.history)\n\n    if args.compare:\n        # Compare two prompts\n        if len(args.compare) != 2:\n            print(\"Error: --compare requires exactly 2 prompts\")\n            return 1\n\n        if not args.query:\n            print(\"Enter your test query (Ctrl+D to finish):\")\n            query = sys.stdin.read().strip()\n        else:\n            query = args.query\n\n        if not query:\n            print(\"Error: No query provided\")\n            return 1\n\n        print(f\"\\nRunning A/B test with query: {query[:50]}...\")\n        print(\"Testing 2 variants...\")\n\n        result = tester.quick_compare(query, args.compare[0], args.compare[1])\n\n        if args.json:\n            output = {\n                \"query\": result.query,\n                \"winner\": result.winner,\n                \"analysis\": result.analysis,\n                \"results\": [\n                    {\n                        \"variant\": r.variant_name,\n                        \"latency_ms\": r.latency_ms,\n                        \"token_count\": r.token_count,\n                        \"scores\": r.scores,\n                        \"response\": r.response[:500] if args.verbose else r.response[:100]\n                    }\n                    for r in result.results\n                ]\n            }\n            print(json.dumps(output, indent=2))\n        else:\n            print(result.summary())\n\n            if args.verbose:\n                print(\"\\nFull Responses:\")\n                for r in result.results:\n                    print(f\"\\n{'-' * 40}\")\n                    print(f\"{r.variant_name}:\")\n                    print(r.response)\n\n        return 0\n\n    elif args.strategies:\n        # Compare preset strategies\n        if not args.query:\n            print(\"Enter your test query (Ctrl+D to finish):\")\n            query = sys.stdin.read().strip()\n        else:\n            query = args.query\n\n        if not query:\n            print(\"Error: No query provided\")\n            return 1\n\n        strategy_names = args.strategies.split(\",\")\n        variants = []\n\n        for name in strategy_names:\n            name = name.strip()\n            if name in PROMPT_STRATEGIES:\n                variants.append(PromptVariant(name=name, prompt=PROMPT_STRATEGIES[name]))\n            else:\n                print(f\"Warning: Unknown strategy '{name}', skipping\")\n\n        if len(variants) < 2:\n            print(\"Error: Need at least 2 valid strategies to compare\")\n            print(f\"Available: {', '.join(PROMPT_STRATEGIES.keys())}\")\n            return 1\n\n        print(f\"\\nRunning A/B test with {len(variants)} strategies...\")\n\n        result = tester.run_test(query, variants)\n\n        if args.json:\n            output = {\n                \"query\": result.query,\n                \"winner\": result.winner,\n                \"analysis\": result.analysis,\n                \"scores_summary\": result.scores_summary\n            }\n            print(json.dumps(output, indent=2))\n        else:\n            print(result.summary())\n\n        return 0\n\n    elif args.stats:\n        # Show stats for a variant\n        stats = tester.get_performance_stats(args.stats)\n\n        if args.json:\n            print(json.dumps(stats, indent=2))\n        else:\n            print(f\"\\nPerformance Stats for '{args.stats}':\")\n            print(\"=\" * 40)\n            if \"error\" in stats:\n                print(stats[\"error\"])\n            else:\n                print(f\"Total Tests: {stats['total_tests']}\")\n                print(f\"Wins: {stats['wins']}\")\n                print(f\"Win Rate: {stats['win_rate']:.1%}\")\n                if stats['avg_quality_score']:\n                    print(f\"Avg Quality Score: {stats['avg_quality_score']:.1f}/10\")\n\n        return 0\n\n    elif args.best:\n        # Show best performing prompts\n        best = tester.get_best_prompts(limit=args.best)\n\n        if args.json:\n            print(json.dumps(best, indent=2))\n        else:\n            print(\"\\nTop Performing Prompt Strategies:\")\n            print(\"=\" * 40)\n            for i, item in enumerate(best, 1):\n                print(f\"{i}. {item['variant']}\")\n                print(f\"   Win Rate: {item['win_rate']:.1%} ({item['wins']}/{item['total_tests']} tests)\")\n\n        return 0\n\n    else:\n        # List available strategies\n        print(\"\\nAvailable Prompt Strategies:\")\n        print(\"=\" * 40)\n        for name, prompt in PROMPT_STRATEGIES.items():\n            preview = prompt.replace(\"\\n\", \" \")[:60]\n            print(f\"  {name}: {preview}...\")\n        print(\"\\nUse --strategies <s1>,<s2> --query <query> to compare strategies\")\n        return 0\n\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"promptwizz\",\n        description=\"PromptWizz - AI Prompt Engineering Toolkit\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s analyze -p \"Write code for me\"\n  %(prog)s score -p \"Create a Python function that sorts a list\"\n  %(prog)s templates --list\n  %(prog)s templates --render code_review --var language=python --var code=\"def foo(): pass\"\n  %(prog)s abtest --strategies direct,expert --query \"Explain recursion\"\n        \"\"\"\n    )\n\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # Analyze command\n    analyze_parser = subparsers.add_parser(\"analyze\", help=\"Analyze a prompt for quality\")\n    analyze_parser.add_argument(\"-p\", \"--prompt\", help=\"Prompt text to analyze\")\n    analyze_parser.add_argument(\"-f\", \"--file\", help=\"Read prompt from file\")\n    analyze_parser.add_argument(\"--ai\", action=\"store_true\", help=\"Use AI for enhanced analysis\")\n    analyze_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # Score command\n    score_parser = subparsers.add_parser(\"score\", help=\"Quick score a prompt\")\n    score_parser.add_argument(\"-p\", \"--prompt\", help=\"Prompt text to score\")\n    score_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # Templates command\n    templates_parser = subparsers.add_parser(\"templates\", help=\"Manage prompt templates\")\n    templates_parser.add_argument(\"--list\", \"-l\", action=\"store_true\", help=\"List all templates\")\n    templates_parser.add_argument(\"--category\", \"-c\", help=\"Filter by category\")\n    templates_parser.add_argument(\"--show\", \"-s\", help=\"Show a specific template\")\n    templates_parser.add_argument(\"--render\", \"-r\", help=\"Render a template\")\n    templates_parser.add_argument(\"--var\", action=\"append\", help=\"Variable for rendering (key=value)\")\n    templates_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # A/B Test command\n    abtest_parser = subparsers.add_parser(\"abtest\", help=\"Run A/B tests on prompts\")\n    abtest_parser.add_argument(\"--compare\", nargs=2, metavar=(\"PROMPT_A\", \"PROMPT_B\"),\n                               help=\"Compare two prompts\")\n    abtest_parser.add_argument(\"--strategies\", help=\"Compare preset strategies (comma-separated)\")\n    abtest_parser.add_argument(\"--query\", \"-q\", help=\"Query to test\")\n    abtest_parser.add_argument(\"--stats\", help=\"Show stats for a variant\")\n    abtest_parser.add_argument(\"--best\", type=int, nargs=\"?\", const=5, help=\"Show top performing prompts\")\n    abtest_parser.add_argument(\"--history\", default=\".promptwizz_history.json\",\n                               help=\"History file path\")\n    abtest_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n    abtest_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Show full responses\")\n\n    args = parser.parse_args()\n\n    if not args.command:\n        parser.print_help()\n        print(\"\\n\" + \"=\" * 60)\n        print(\"PromptWizz - AI Prompt Engineering Toolkit\")\n        print(\"=\" * 60)\n        print(\"\"\"\nCommands:\n  analyze    - Analyze prompts for quality and get improvement suggestions\n  score      - Quick score a prompt (0-100)\n  templates  - Browse and use optimized prompt templates\n  abtest     - A/B test different prompt strategies\n\nGet started:\n  promptwizz analyze -p \"Your prompt here\"\n  promptwizz templates --list\n        \"\"\")\n        return 0\n\n    # Route to appropriate command handler\n    handlers = {\n        \"analyze\": cmd_analyze,\n        \"score\": cmd_score,\n        \"templates\": cmd_templates,\n        \"abtest\": cmd_abtest,\n    }\n\n    handler = handlers.get(args.command)\n    if handler:\n        return handler(args)\n    else:\n        parser.print_help()\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":390,"cache_read_input_tokens":18517,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":390},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgd6J5FTCieKBiUBFqte","type":"assistant","uuid":"6d0d8f9e-0017-4636-8b99-302f58a02da7","timestamp":"2026-02-01T03:19:53.692Z"}
{"parentUuid":"6d0d8f9e-0017-4636-8b99-302f58a02da7","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_0162z8PKmgJZ54XGyWDRqXga","toolUseID":"toolu_0162z8PKmgJZ54XGyWDRqXga","timestamp":"2026-02-01T03:19:53.742Z","uuid":"08dc1887-0dd1-48ad-beea-f6afdf056939"}
{"parentUuid":"6d0d8f9e-0017-4636-8b99-302f58a02da7","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0162z8PKmgJZ54XGyWDRqXga","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/cli.py"}]},"uuid":"4dbc570a-0dc6-41c2-b91f-84f26558d355","timestamp":"2026-02-01T03:19:53.738Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/cli.py","content":"#!/usr/bin/env python3\n\"\"\"\nPromptWizz CLI\n\nCommand-line interface for the PromptWizz prompt engineering toolkit.\nProvides access to prompt analysis, templates, and A/B testing.\n\"\"\"\n\nimport argparse\nimport json\nimport sys\nfrom pathlib import Path\n\nfrom analyzer import PromptAnalyzer, analyze_prompt\nfrom templates import TemplateLibrary, TemplateCategory, library\nfrom ab_test import ABTester, PromptVariant, PROMPT_STRATEGIES\n\n\ndef cmd_analyze(args):\n    \"\"\"Analyze a prompt for quality.\"\"\"\n    # Get prompt from args or stdin\n    if args.prompt:\n        prompt = args.prompt\n    elif args.file:\n        prompt = Path(args.file).read_text()\n    else:\n        print(\"Enter your prompt (Ctrl+D to finish):\")\n        prompt = sys.stdin.read()\n\n    if not prompt.strip():\n        print(\"Error: No prompt provided\")\n        return 1\n\n    print(\"\\nAnalyzing prompt...\")\n    analyzer = PromptAnalyzer(use_ai=args.ai)\n    result = analyzer.analyze(prompt)\n\n    if args.json:\n        output = {\n            \"original_prompt\": result.original_prompt,\n            \"overall_score\": result.overall_score,\n            \"scores\": result.scores,\n            \"weaknesses\": [\n                {\"category\": w.category, \"description\": w.description, \"severity\": w.severity}\n                for w in result.weaknesses\n            ],\n            \"improvements\": [\n                {\"category\": i.category, \"suggestion\": i.suggestion, \"priority\": i.priority}\n                for i in result.improvements\n            ],\n            \"improved_prompt\": result.improved_prompt\n        }\n        print(json.dumps(output, indent=2))\n    else:\n        print(result.summary())\n\n        if result.improved_prompt:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"IMPROVED PROMPT:\")\n            print(\"=\" * 60)\n            print(result.improved_prompt)\n\n    return 0\n\n\ndef cmd_score(args):\n    \"\"\"Quick score a prompt.\"\"\"\n    if args.prompt:\n        prompt = args.prompt\n    else:\n        print(\"Enter your prompt (Ctrl+D to finish):\")\n        prompt = sys.stdin.read()\n\n    if not prompt.strip():\n        print(\"Error: No prompt provided\")\n        return 1\n\n    analyzer = PromptAnalyzer(use_ai=False)\n    score = analyzer.quick_score(prompt)\n\n    if args.json:\n        print(json.dumps({\"score\": score}))\n    else:\n        bar = \"█\" * int(score / 10) + \"░\" * (10 - int(score / 10))\n        print(f\"\\nPrompt Score: [{bar}] {score:.1f}/100\")\n\n        if score < 40:\n            print(\"Quality: Poor - Needs significant improvement\")\n        elif score < 60:\n            print(\"Quality: Fair - Has room for improvement\")\n        elif score < 80:\n            print(\"Quality: Good - Solid prompt with minor issues\")\n        else:\n            print(\"Quality: Excellent - Well-crafted prompt\")\n\n    return 0\n\n\ndef cmd_templates(args):\n    \"\"\"List or show templates.\"\"\"\n    if args.list:\n        # List all templates\n        category = TemplateCategory(args.category) if args.category else None\n        templates = library.list_templates(category=category)\n\n        if args.json:\n            output = [\n                {\n                    \"name\": t.name,\n                    \"category\": t.category.value,\n                    \"description\": t.description,\n                    \"variables\": t.variables,\n                    \"tags\": t.tags\n                }\n                for t in templates\n            ]\n            print(json.dumps(output, indent=2))\n        else:\n            print(\"\\nAvailable Templates:\")\n            print(\"=\" * 60)\n\n            current_cat = None\n            for t in sorted(templates, key=lambda x: (x.category.value, x.name)):\n                if t.category != current_cat:\n                    current_cat = t.category\n                    print(f\"\\n[{current_cat.value.upper()}]\")\n\n                vars_str = \", \".join(t.variables[:3])\n                if len(t.variables) > 3:\n                    vars_str += \"...\"\n\n                print(f\"  {t.name}\")\n                print(f\"    {t.description}\")\n                print(f\"    Variables: {vars_str}\")\n\n        return 0\n\n    elif args.show:\n        # Show specific template\n        template = library.get(args.show)\n\n        if not template:\n            print(f\"Error: Template '{args.show}' not found\")\n            return 1\n\n        if args.json:\n            print(json.dumps(template.to_dict(), indent=2))\n        else:\n            print(f\"\\nTemplate: {template.name}\")\n            print(\"=\" * 60)\n            print(f\"Category: {template.category.value}\")\n            print(f\"Description: {template.description}\")\n            print(f\"\\nRequired Variables: {', '.join(template.variables)}\")\n            if template.optional_variables:\n                opt_vars = [f\"{k} (default: {v})\" for k, v in template.optional_variables.items()]\n                print(f\"Optional Variables: {', '.join(opt_vars)}\")\n            print(f\"Tags: {', '.join(template.tags)}\")\n            print(f\"\\nTemplate:\")\n            print(\"-\" * 40)\n            print(template.template)\n\n        return 0\n\n    elif args.render:\n        # Render a template with variables\n        template = library.get(args.render)\n\n        if not template:\n            print(f\"Error: Template '{args.render}' not found\")\n            return 1\n\n        # Parse variables from --var key=value arguments\n        variables = {}\n        if args.var:\n            for var_str in args.var:\n                if \"=\" in var_str:\n                    key, value = var_str.split(\"=\", 1)\n                    variables[key.strip()] = value.strip()\n\n        try:\n            rendered = template.render(**variables)\n\n            if args.json:\n                print(json.dumps({\"prompt\": rendered}))\n            else:\n                print(\"\\nRendered Prompt:\")\n                print(\"=\" * 60)\n                print(rendered)\n\n            return 0\n\n        except ValueError as e:\n            print(f\"Error: {e}\")\n            print(f\"\\nRequired variables: {', '.join(template.variables)}\")\n            return 1\n\n    else:\n        # Default: list categories\n        print(\"\\nTemplate Categories:\")\n        for cat in library.get_categories():\n            count = len(library.list_templates(category=cat))\n            print(f\"  {cat.value}: {count} templates\")\n        print(\"\\nUse --list to see all templates or --show <name> to view a specific template\")\n        return 0\n\n\ndef cmd_abtest(args):\n    \"\"\"Run A/B tests on prompts.\"\"\"\n    tester = ABTester(history_file=args.history)\n\n    if args.compare:\n        # Compare two prompts\n        if len(args.compare) != 2:\n            print(\"Error: --compare requires exactly 2 prompts\")\n            return 1\n\n        if not args.query:\n            print(\"Enter your test query (Ctrl+D to finish):\")\n            query = sys.stdin.read().strip()\n        else:\n            query = args.query\n\n        if not query:\n            print(\"Error: No query provided\")\n            return 1\n\n        print(f\"\\nRunning A/B test with query: {query[:50]}...\")\n        print(\"Testing 2 variants...\")\n\n        result = tester.quick_compare(query, args.compare[0], args.compare[1])\n\n        if args.json:\n            output = {\n                \"query\": result.query,\n                \"winner\": result.winner,\n                \"analysis\": result.analysis,\n                \"results\": [\n                    {\n                        \"variant\": r.variant_name,\n                        \"latency_ms\": r.latency_ms,\n                        \"token_count\": r.token_count,\n                        \"scores\": r.scores,\n                        \"response\": r.response[:500] if args.verbose else r.response[:100]\n                    }\n                    for r in result.results\n                ]\n            }\n            print(json.dumps(output, indent=2))\n        else:\n            print(result.summary())\n\n            if args.verbose:\n                print(\"\\nFull Responses:\")\n                for r in result.results:\n                    print(f\"\\n{'-' * 40}\")\n                    print(f\"{r.variant_name}:\")\n                    print(r.response)\n\n        return 0\n\n    elif args.strategies:\n        # Compare preset strategies\n        if not args.query:\n            print(\"Enter your test query (Ctrl+D to finish):\")\n            query = sys.stdin.read().strip()\n        else:\n            query = args.query\n\n        if not query:\n            print(\"Error: No query provided\")\n            return 1\n\n        strategy_names = args.strategies.split(\",\")\n        variants = []\n\n        for name in strategy_names:\n            name = name.strip()\n            if name in PROMPT_STRATEGIES:\n                variants.append(PromptVariant(name=name, prompt=PROMPT_STRATEGIES[name]))\n            else:\n                print(f\"Warning: Unknown strategy '{name}', skipping\")\n\n        if len(variants) < 2:\n            print(\"Error: Need at least 2 valid strategies to compare\")\n            print(f\"Available: {', '.join(PROMPT_STRATEGIES.keys())}\")\n            return 1\n\n        print(f\"\\nRunning A/B test with {len(variants)} strategies...\")\n\n        result = tester.run_test(query, variants)\n\n        if args.json:\n            output = {\n                \"query\": result.query,\n                \"winner\": result.winner,\n                \"analysis\": result.analysis,\n                \"scores_summary\": result.scores_summary\n            }\n            print(json.dumps(output, indent=2))\n        else:\n            print(result.summary())\n\n        return 0\n\n    elif args.stats:\n        # Show stats for a variant\n        stats = tester.get_performance_stats(args.stats)\n\n        if args.json:\n            print(json.dumps(stats, indent=2))\n        else:\n            print(f\"\\nPerformance Stats for '{args.stats}':\")\n            print(\"=\" * 40)\n            if \"error\" in stats:\n                print(stats[\"error\"])\n            else:\n                print(f\"Total Tests: {stats['total_tests']}\")\n                print(f\"Wins: {stats['wins']}\")\n                print(f\"Win Rate: {stats['win_rate']:.1%}\")\n                if stats['avg_quality_score']:\n                    print(f\"Avg Quality Score: {stats['avg_quality_score']:.1f}/10\")\n\n        return 0\n\n    elif args.best:\n        # Show best performing prompts\n        best = tester.get_best_prompts(limit=args.best)\n\n        if args.json:\n            print(json.dumps(best, indent=2))\n        else:\n            print(\"\\nTop Performing Prompt Strategies:\")\n            print(\"=\" * 40)\n            for i, item in enumerate(best, 1):\n                print(f\"{i}. {item['variant']}\")\n                print(f\"   Win Rate: {item['win_rate']:.1%} ({item['wins']}/{item['total_tests']} tests)\")\n\n        return 0\n\n    else:\n        # List available strategies\n        print(\"\\nAvailable Prompt Strategies:\")\n        print(\"=\" * 40)\n        for name, prompt in PROMPT_STRATEGIES.items():\n            preview = prompt.replace(\"\\n\", \" \")[:60]\n            print(f\"  {name}: {preview}...\")\n        print(\"\\nUse --strategies <s1>,<s2> --query <query> to compare strategies\")\n        return 0\n\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"promptwizz\",\n        description=\"PromptWizz - AI Prompt Engineering Toolkit\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s analyze -p \"Write code for me\"\n  %(prog)s score -p \"Create a Python function that sorts a list\"\n  %(prog)s templates --list\n  %(prog)s templates --render code_review --var language=python --var code=\"def foo(): pass\"\n  %(prog)s abtest --strategies direct,expert --query \"Explain recursion\"\n        \"\"\"\n    )\n\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # Analyze command\n    analyze_parser = subparsers.add_parser(\"analyze\", help=\"Analyze a prompt for quality\")\n    analyze_parser.add_argument(\"-p\", \"--prompt\", help=\"Prompt text to analyze\")\n    analyze_parser.add_argument(\"-f\", \"--file\", help=\"Read prompt from file\")\n    analyze_parser.add_argument(\"--ai\", action=\"store_true\", help=\"Use AI for enhanced analysis\")\n    analyze_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # Score command\n    score_parser = subparsers.add_parser(\"score\", help=\"Quick score a prompt\")\n    score_parser.add_argument(\"-p\", \"--prompt\", help=\"Prompt text to score\")\n    score_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # Templates command\n    templates_parser = subparsers.add_parser(\"templates\", help=\"Manage prompt templates\")\n    templates_parser.add_argument(\"--list\", \"-l\", action=\"store_true\", help=\"List all templates\")\n    templates_parser.add_argument(\"--category\", \"-c\", help=\"Filter by category\")\n    templates_parser.add_argument(\"--show\", \"-s\", help=\"Show a specific template\")\n    templates_parser.add_argument(\"--render\", \"-r\", help=\"Render a template\")\n    templates_parser.add_argument(\"--var\", action=\"append\", help=\"Variable for rendering (key=value)\")\n    templates_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n\n    # A/B Test command\n    abtest_parser = subparsers.add_parser(\"abtest\", help=\"Run A/B tests on prompts\")\n    abtest_parser.add_argument(\"--compare\", nargs=2, metavar=(\"PROMPT_A\", \"PROMPT_B\"),\n                               help=\"Compare two prompts\")\n    abtest_parser.add_argument(\"--strategies\", help=\"Compare preset strategies (comma-separated)\")\n    abtest_parser.add_argument(\"--query\", \"-q\", help=\"Query to test\")\n    abtest_parser.add_argument(\"--stats\", help=\"Show stats for a variant\")\n    abtest_parser.add_argument(\"--best\", type=int, nargs=\"?\", const=5, help=\"Show top performing prompts\")\n    abtest_parser.add_argument(\"--history\", default=\".promptwizz_history.json\",\n                               help=\"History file path\")\n    abtest_parser.add_argument(\"--json\", action=\"store_true\", help=\"Output in JSON format\")\n    abtest_parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Show full responses\")\n\n    args = parser.parse_args()\n\n    if not args.command:\n        parser.print_help()\n        print(\"\\n\" + \"=\" * 60)\n        print(\"PromptWizz - AI Prompt Engineering Toolkit\")\n        print(\"=\" * 60)\n        print(\"\"\"\nCommands:\n  analyze    - Analyze prompts for quality and get improvement suggestions\n  score      - Quick score a prompt (0-100)\n  templates  - Browse and use optimized prompt templates\n  abtest     - A/B test different prompt strategies\n\nGet started:\n  promptwizz analyze -p \"Your prompt here\"\n  promptwizz templates --list\n        \"\"\")\n        return 0\n\n    # Route to appropriate command handler\n    handlers = {\n        \"analyze\": cmd_analyze,\n        \"score\": cmd_score,\n        \"templates\": cmd_templates,\n        \"abtest\": cmd_abtest,\n    }\n\n    handler = handlers.get(args.command)\n    if handler:\n        return handler(args)\n    else:\n        parser.print_help()\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"6d0d8f9e-0017-4636-8b99-302f58a02da7"}
{"parentUuid":"4dbc570a-0dc6-41c2-b91f-84f26558d355","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MpUSbJKmfqPsgKgS8fWNG2","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Now let me add the requirements file and a setup.py for the package, then test that the imports work.","signature":"Eo0CCkYICxgCKkCwKvvmuRsCM5cK9QyZi0jkd73+KloOnGo2NhL5YhDyNoD5GAkoqSgsQX2A4LMo4f48yPdH246zKvt0HWbIHykLEgxwp6ZfTcuP4EdO/mwaDH8vcuBf4UVLWmocMCIwIM17+D1vZA/pBv7c5Eg0enuhE/uLeLbdVAW+lN5idwyFq20nX7U3qqZGP8uwg12RKnWUnI/Ff5jchC8wAVe7agrCkOZvnlFXTBtIShZMZm5p+lvSSqVM5XJPxRP5JN5CaZJWa/dkCsO1bWBFM6vDNvr5Rvcl/xoEljaKpEj1pesvf9+k2pZXKOU+f4aHWN/0bn/amUc+ig+d7lqwyXeU58IfZ63Zip8YAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":18392,"cache_read_input_tokens":18907,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":18392},"output_tokens":4,"service_tier":"standard"}},"requestId":"req_011CXgdNfssXAu1Wif7VGZza","type":"assistant","uuid":"299f97fd-4e9f-41f2-a7d5-678b2ff2382d","timestamp":"2026-02-01T03:19:56.463Z"}
{"parentUuid":"299f97fd-4e9f-41f2-a7d5-678b2ff2382d","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MpUSbJKmfqPsgKgS8fWNG2","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me add the package configuration files and verify everything works."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":18392,"cache_read_input_tokens":18907,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":18392},"output_tokens":4,"service_tier":"standard"}},"requestId":"req_011CXgdNfssXAu1Wif7VGZza","type":"assistant","uuid":"149791e0-2716-4a49-a1b8-09a775a31eb5","timestamp":"2026-02-01T03:19:57.009Z"}
{"type":"file-history-snapshot","messageId":"a676bcba-3274-4d78-9af8-ceed8bf2cf4d","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"},"ab_test.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:09.481Z"},"cli.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:53.717Z"},"requirements.txt":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:57.696Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"149791e0-2716-4a49-a1b8-09a775a31eb5","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MpUSbJKmfqPsgKgS8fWNG2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PJ1EWhcRyMSeTPUw6joEtJ","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/requirements.txt","content":"anthropic>=0.18.0\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":18392,"cache_read_input_tokens":18907,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":18392},"output_tokens":4,"service_tier":"standard"}},"requestId":"req_011CXgdNfssXAu1Wif7VGZza","type":"assistant","uuid":"a676bcba-3274-4d78-9af8-ceed8bf2cf4d","timestamp":"2026-02-01T03:19:57.677Z"}
{"parentUuid":"a676bcba-3274-4d78-9af8-ceed8bf2cf4d","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01PJ1EWhcRyMSeTPUw6joEtJ","toolUseID":"toolu_01PJ1EWhcRyMSeTPUw6joEtJ","timestamp":"2026-02-01T03:19:57.728Z","uuid":"905c1a28-0d7f-4b6c-bfbc-e2ca3b352f8f"}
{"parentUuid":"a676bcba-3274-4d78-9af8-ceed8bf2cf4d","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PJ1EWhcRyMSeTPUw6joEtJ","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/requirements.txt"}]},"uuid":"a88a6747-e688-42d8-8a27-69c21588827c","timestamp":"2026-02-01T03:19:57.725Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/requirements.txt","content":"anthropic>=0.18.0\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"a676bcba-3274-4d78-9af8-ceed8bf2cf4d"}
{"type":"file-history-snapshot","messageId":"05cde406-34b8-4f70-82bb-d7adedc06630","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"},"ab_test.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:09.481Z"},"cli.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:53.717Z"},"requirements.txt":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:57.696Z"},"pyproject.toml":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:20:04.145Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"a88a6747-e688-42d8-8a27-69c21588827c","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MpUSbJKmfqPsgKgS8fWNG2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GCKwkmLDHz7tJuzcF2ZASE","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/pyproject.toml","content":"[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"promptwizz-tools\"\nversion = \"1.0.0\"\ndescription = \"AI Prompt Engineering Toolkit for PromptWizz.com\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\nrequires-python = \">=3.10\"\nauthors = [\n    {name = \"Matt\", email = \"matt@promptwizz.com\"}\n]\nkeywords = [\"prompt-engineering\", \"ai\", \"claude\", \"llm\", \"anthropic\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n]\ndependencies = [\n    \"anthropic>=0.18.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0\",\n    \"pytest-cov>=4.0\",\n    \"black>=23.0\",\n    \"ruff>=0.1.0\",\n]\n\n[project.scripts]\npromptwizz = \"cli:main\"\n\n[project.urls]\nHomepage = \"https://promptwizz.com\"\nRepository = \"https://github.com/promptwizz/promptwizz-tools\"\n\n[tool.setuptools]\npy-modules = [\"analyzer\", \"templates\", \"ab_test\", \"cli\"]\n\n[tool.black]\nline-length = 100\ntarget-version = ['py310', 'py311', 'py312']\n\n[tool.ruff]\nline-length = 100\nselect = [\"E\", \"F\", \"W\", \"I\", \"N\", \"UP\", \"B\", \"C4\"]\nignore = [\"E501\"]\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":18392,"cache_read_input_tokens":18907,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":18392},"output_tokens":4,"service_tier":"standard"}},"requestId":"req_011CXgdNfssXAu1Wif7VGZza","type":"assistant","uuid":"05cde406-34b8-4f70-82bb-d7adedc06630","timestamp":"2026-02-01T03:20:04.130Z"}
{"parentUuid":"05cde406-34b8-4f70-82bb-d7adedc06630","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01GCKwkmLDHz7tJuzcF2ZASE","toolUseID":"toolu_01GCKwkmLDHz7tJuzcF2ZASE","timestamp":"2026-02-01T03:20:04.173Z","uuid":"260631e0-74c0-4b5b-b7dd-8e8be24c82fc"}
{"parentUuid":"05cde406-34b8-4f70-82bb-d7adedc06630","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GCKwkmLDHz7tJuzcF2ZASE","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/pyproject.toml"}]},"uuid":"2d615014-1f3c-4c0a-8cd2-2726445fd879","timestamp":"2026-02-01T03:20:04.169Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/pyproject.toml","content":"[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"promptwizz-tools\"\nversion = \"1.0.0\"\ndescription = \"AI Prompt Engineering Toolkit for PromptWizz.com\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\nrequires-python = \">=3.10\"\nauthors = [\n    {name = \"Matt\", email = \"matt@promptwizz.com\"}\n]\nkeywords = [\"prompt-engineering\", \"ai\", \"claude\", \"llm\", \"anthropic\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n]\ndependencies = [\n    \"anthropic>=0.18.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0\",\n    \"pytest-cov>=4.0\",\n    \"black>=23.0\",\n    \"ruff>=0.1.0\",\n]\n\n[project.scripts]\npromptwizz = \"cli:main\"\n\n[project.urls]\nHomepage = \"https://promptwizz.com\"\nRepository = \"https://github.com/promptwizz/promptwizz-tools\"\n\n[tool.setuptools]\npy-modules = [\"analyzer\", \"templates\", \"ab_test\", \"cli\"]\n\n[tool.black]\nline-length = 100\ntarget-version = ['py310', 'py311', 'py312']\n\n[tool.ruff]\nline-length = 100\nselect = [\"E\", \"F\", \"W\", \"I\", \"N\", \"UP\", \"B\", \"C4\"]\nignore = [\"E501\"]\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"05cde406-34b8-4f70-82bb-d7adedc06630"}
{"type":"file-history-snapshot","messageId":"60636d8e-bab5-4dd5-a04d-c64ad0989e12","snapshot":{"messageId":"9c7ce2c6-099f-4862-9444-ea9c8cf43007","trackedFileBackups":{"analyzer.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:17:14.399Z"},"templates.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:18:19.793Z"},"ab_test.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:09.481Z"},"cli.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:53.717Z"},"requirements.txt":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:19:57.696Z"},"pyproject.toml":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:20:04.145Z"},"__init__.py":{"backupFileName":null,"version":1,"backupTime":"2026-02-01T03:20:11.339Z"}},"timestamp":"2026-02-01T03:16:06.090Z"},"isSnapshotUpdate":true}
{"parentUuid":"2d615014-1f3c-4c0a-8cd2-2726445fd879","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01MpUSbJKmfqPsgKgS8fWNG2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01G5pAvyo1KXiEWeEsfaZ4Pk","name":"Write","input":{"file_path":"/workspace/projects/promptwizz-tools/__init__.py","content":"\"\"\"\nPromptWizz Tools - AI Prompt Engineering Toolkit\n\nA comprehensive toolkit for analyzing, optimizing, and testing AI prompts.\n\nModules:\n    analyzer: Prompt quality analysis and improvement suggestions\n    templates: Library of optimized prompt templates\n    ab_test: A/B testing framework for comparing prompts\n    cli: Command-line interface\n\nExample:\n    >>> from analyzer import analyze_prompt\n    >>> result = analyze_prompt(\"Write me something good\")\n    >>> print(f\"Score: {result.overall_score}\")\n\n    >>> from templates import library\n    >>> prompt = library.render(\"code_review\", language=\"python\", code=\"def foo(): pass\")\n\n    >>> from ab_test import ABTester\n    >>> tester = ABTester()\n    >>> result = tester.quick_compare(query, prompt_a, prompt_b)\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Matt\"\n__email__ = \"matt@promptwizz.com\"\n\nfrom analyzer import PromptAnalyzer, analyze_prompt, AnalysisResult\nfrom templates import TemplateLibrary, PromptTemplate, library, render_template\nfrom ab_test import ABTester, PromptVariant, ComparisonResult\n\n__all__ = [\n    # Analyzer\n    \"PromptAnalyzer\",\n    \"analyze_prompt\",\n    \"AnalysisResult\",\n    # Templates\n    \"TemplateLibrary\",\n    \"PromptTemplate\",\n    \"library\",\n    \"render_template\",\n    # A/B Testing\n    \"ABTester\",\n    \"PromptVariant\",\n    \"ComparisonResult\",\n]\n"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":18392,"cache_read_input_tokens":18907,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":18392},"output_tokens":4,"service_tier":"standard"}},"requestId":"req_011CXgdNfssXAu1Wif7VGZza","type":"assistant","uuid":"60636d8e-bab5-4dd5-a04d-c64ad0989e12","timestamp":"2026-02-01T03:20:11.322Z"}
{"parentUuid":"60636d8e-bab5-4dd5-a04d-c64ad0989e12","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Write","command":"callback"},"parentToolUseID":"toolu_01G5pAvyo1KXiEWeEsfaZ4Pk","toolUseID":"toolu_01G5pAvyo1KXiEWeEsfaZ4Pk","timestamp":"2026-02-01T03:20:11.369Z","uuid":"ffeb9339-3765-4938-b976-bb3ff8560ca3"}
{"parentUuid":"60636d8e-bab5-4dd5-a04d-c64ad0989e12","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01G5pAvyo1KXiEWeEsfaZ4Pk","type":"tool_result","content":"File created successfully at: /workspace/projects/promptwizz-tools/__init__.py"}]},"uuid":"89fe3421-b5ab-40cd-8467-eee641b6a705","timestamp":"2026-02-01T03:20:11.365Z","toolUseResult":{"type":"create","filePath":"/workspace/projects/promptwizz-tools/__init__.py","content":"\"\"\"\nPromptWizz Tools - AI Prompt Engineering Toolkit\n\nA comprehensive toolkit for analyzing, optimizing, and testing AI prompts.\n\nModules:\n    analyzer: Prompt quality analysis and improvement suggestions\n    templates: Library of optimized prompt templates\n    ab_test: A/B testing framework for comparing prompts\n    cli: Command-line interface\n\nExample:\n    >>> from analyzer import analyze_prompt\n    >>> result = analyze_prompt(\"Write me something good\")\n    >>> print(f\"Score: {result.overall_score}\")\n\n    >>> from templates import library\n    >>> prompt = library.render(\"code_review\", language=\"python\", code=\"def foo(): pass\")\n\n    >>> from ab_test import ABTester\n    >>> tester = ABTester()\n    >>> result = tester.quick_compare(query, prompt_a, prompt_b)\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Matt\"\n__email__ = \"matt@promptwizz.com\"\n\nfrom analyzer import PromptAnalyzer, analyze_prompt, AnalysisResult\nfrom templates import TemplateLibrary, PromptTemplate, library, render_template\nfrom ab_test import ABTester, PromptVariant, ComparisonResult\n\n__all__ = [\n    # Analyzer\n    \"PromptAnalyzer\",\n    \"analyze_prompt\",\n    \"AnalysisResult\",\n    # Templates\n    \"TemplateLibrary\",\n    \"PromptTemplate\",\n    \"library\",\n    \"render_template\",\n    # A/B Testing\n    \"ABTester\",\n    \"PromptVariant\",\n    \"ComparisonResult\",\n]\n","structuredPatch":[],"originalFile":null},"sourceToolAssistantUUID":"60636d8e-bab5-4dd5-a04d-c64ad0989e12"}
{"parentUuid":"89fe3421-b5ab-40cd-8467-eee641b6a705","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FjQwLnGViQoAABZLyGWz2z","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Now let me verify the modules work by running a quick test.","signature":"EuMBCkYICxgCKkAKg1aQ3PN2Wa689X/qKpD+vytC83gr7+VgkETfD7GmVCIU9PCuIIk9E3ZEzQ9/YAiMssyS6SOx9qqHIKdim3G/Egy6rV85qW6DK8JLEtYaDB9RyeYsC0uoWPjZniIwoNM+qkDe8RuDj6ISvv+XMajQUXF8jbZIJFMCMEvqaEvAL8e8svD+hUFcFS9gfC6UKkvSzvXQUVNilVkKjkZ38qs0NRPi3T5HAHqtg/AtcNkapf4CCMj3FTBJWdgYdPkPxweE79//Fac9EtQhqHF5fg/zndAHBYzQZM9uKJkYAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1428,"cache_read_input_tokens":37299,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1428},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgdPxz3fuUqNd8ieVYaf","type":"assistant","uuid":"02733f5e-4db7-45fb-80be-011c61f4280a","timestamp":"2026-02-01T03:20:14.190Z"}
{"parentUuid":"02733f5e-4db7-45fb-80be-011c61f4280a","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FjQwLnGViQoAABZLyGWz2z","type":"message","role":"assistant","content":[{"type":"text","text":"Let me verify the toolkit works correctly."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1428,"cache_read_input_tokens":37299,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1428},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgdPxz3fuUqNd8ieVYaf","type":"assistant","uuid":"5b72e28d-9528-40ba-af76-1202cfe058ac","timestamp":"2026-02-01T03:20:14.727Z"}
{"parentUuid":"5b72e28d-9528-40ba-af76-1202cfe058ac","isSidechain":false,"userType":"external","cwd":"/workspace/projects/promptwizz-tools","sessionId":"4a29c807-8ba9-46d5-b4ce-813f891e569d","version":"2.1.29","gitBranch":"","slug":"scalable-crafting-thacker","message":{"model":"claude-opus-4-5-20251101","id":"msg_01FjQwLnGViQoAABZLyGWz2z","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014tsRGwDQFRc8mH5whY7jHk","name":"Bash","input":{"command":"python3 -c \"\nfrom analyzer import PromptAnalyzer, analyze_prompt\nfrom templates import library, TemplateCategory\nfrom ab_test import ABTester, PROMPT_STRATEGIES\n\n# Test analyzer\nanalyzer = PromptAnalyzer(use_ai=False)\nresult = analyzer.analyze('Write me something good about Python')\nprint('Analyzer test:')\nprint(f'  Overall score: {result.overall_score:.1f}/100')\nprint(f'  Weaknesses found: {len(result.weaknesses)}')\n\n# Test templates\nprint(f'\\nTemplates test:')\nprint(f'  Total templates: {len(library.templates)}')\nfor cat in library.get_categories():\n    count = len(library.list_templates(category=cat))\n    print(f'  {cat.value}: {count}')\n\n# Test template rendering\nprompt = library.render('code_explain', language='python', code='def foo(): pass')\nprint(f'\\n  Rendered prompt length: {len(prompt)} chars')\n\n# Test A/B strategies\nprint(f'\\nA/B Test strategies: {len(PROMPT_STRATEGIES)}')\nfor name in PROMPT_STRATEGIES:\n    print(f'  - {name}')\n\nprint('\\nAll modules loaded successfully!')\n\"\n","description":"Verify all modules work correctly"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1428,"cache_read_input_tokens":37299,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1428},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CXgdPxz3fuUqNd8ieVYaf","type":"assistant","uuid":"d2600274-ff5d-4038-92c5-f4a29c7236c4","timestamp":"2026-02-01T03:20:20.853Z"}
